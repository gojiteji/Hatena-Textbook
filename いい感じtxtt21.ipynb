{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN1EF21wvSzXaHbjOD/96Jg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gojiteji/Hatena-Textbook/blob/master/%E3%81%84%E3%81%84%E6%84%9F%E3%81%98txtt21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DFhz5alN4bn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "4827a764-f6fc-482a-f2bf-3d698b29cad5"
      },
      "source": [
        "!mkdir /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 777 /root/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c nlp-getting-started"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/411k [00:00<?, ?B/s]\n",
            "100% 411k/411k [00:00<00:00, 62.1MB/s]\n",
            "Downloading train.csv to /content\n",
            "  0% 0.00/965k [00:00<?, ?B/s]\n",
            "100% 965k/965k [00:00<00:00, 63.9MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/22.2k [00:00<?, ?B/s]\n",
            "100% 22.2k/22.2k [00:00<00:00, 22.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OglfQWKOG80",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8a35a683-625d-4712-9a75-0cf176d09ca3"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize ,TweetTokenizer\n",
        "import torch.optim as optimizers\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "!pip install tqdm\n",
        "from tqdm import tqdm\n",
        "warnings.simplefilter('ignore')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "rus = RandomUnderSampler()\n",
        "#辞書構造\n",
        "class Vocabulary():\n",
        "    def __init__(self,disabletokenize=False):\n",
        "        self.w2i={}\n",
        "        self.i2w={}\n",
        "        self.oov_char='<unk>'\n",
        "        self.pad='<pad>'\n",
        "        self.special_chars = [self.pad,self.oov_char]\n",
        "        self.data = \"\"\n",
        "        self._words=set()\n",
        "        self.tokenize=disabletokenize\n",
        "        self.tknzr = TweetTokenizer()\n",
        "    def update(self,text):\n",
        "        if(self.tokenize):\n",
        "            self._words.update(text)\n",
        "        else:\n",
        "            self.data=self.tknzr.tokenize(text)\n",
        "            #self.data=word_tokenize(text)\n",
        "            self._words.update(self.data)\n",
        "        self.w2i = {w: (i + len(self.special_chars)) for i, w in enumerate(self._words)}\n",
        "\n",
        "        self.i2w = {i: w for w, i in self.w2i.items()}\n",
        "        self.w2i['<pad>'] = 0\n",
        "        self.i2w[0] = '<pad>'\n",
        "        self.w2i['<unk>'] = 1\n",
        "        self.i2w[1] = '<unk>'\n",
        "\n",
        "\n",
        "    def encode(self,words):\n",
        "        output=[]\n",
        "        if(self.tokenize):\n",
        "            pass\n",
        "        else:\n",
        "            #words=word_tokenize(words)\n",
        "            words=self.tknzr.tokenize(words)\n",
        "        for word in words:\n",
        "            #辞書になし\n",
        "            if word not in self.w2i:\n",
        "                index = self.w2i[self.oov_char]#既存の<unk>を返す\n",
        "            else:\n",
        "            #辞書にあり\n",
        "                index = self.w2i[word]#idを引っ張ってくる\n",
        "            output.append(index)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def decode(self,indexes):#使いどころないけど確認用\n",
        "        out=[]\n",
        "        for index in indexes:\n",
        "            out.append(self.i2w[index])\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv('train.csv')\n",
        "keyword_voc=Vocabulary(disabletokenize=True)\n",
        "keywords = np.array(df.iloc[:,[1]].fillna('nan'))\n",
        "keywords=keywords.reshape(len(keywords))\n",
        "keyword_voc.update(list(keywords))#キーワードを辞書に登録\n",
        "text_voc=Vocabulary()\n",
        "text = np.array(df.iloc[:,[3]])\n",
        "text=text.reshape(len(text))\n",
        "\n",
        "x=np.stack([text, keywords], 1)\n",
        "y = np.array(df.iloc[:,4])\n",
        "raw_X_train, X_test, raw_y_train, y_test = train_test_split(x, y,test_size=0.3,shuffle=True,stratify=y)\n",
        "\n",
        "#trainデータを辞書に登録\n",
        "for t in raw_X_train:\n",
        "    text_voc.update(t[0])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeEJexdexCkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#testデータをtrainデータ内の単語でid化\n",
        "maxlen=158\n",
        "id_=0\n",
        "x_test=np.array([[0]*maxlen]*len(X_test))\n",
        "x_test_key=np.array([0]*len(X_test))\n",
        "for t in X_test:\n",
        "    tmp=text_voc.encode(t[0])\n",
        "    tmp2=keyword_voc.encode([t[1]])\n",
        "    for i in range(maxlen-len(tmp)):\n",
        "        tmp.append(0)\n",
        "    x_test[id_]=tmp\n",
        "    x_test_key[id_]=tmp2[0]\n",
        "    id_=id_+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76m46ZUlOLWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self,d_k,device='cuda'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.scaler = np.sqrt(d_k)\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        '''\n",
        "        # Argument\n",
        "            q, k, v: (batch, sequence, out_features)\n",
        "            mask:    (batch, sequence)\n",
        "        '''\n",
        "        score = torch.einsum('ijk,ilk->ijl', (q, k)) / self.scaler\n",
        "        score = score - torch.max(score, dim=-1, keepdim=True)[0]\n",
        "        score = torch.exp(score)\n",
        "        if mask is not None:\n",
        "            if len(mask.size()) == 2:\n",
        "                mask = mask.unsqueeze(1).repeat(1, score.size(1), 1)\n",
        "            score.data.masked_fill_(mask, 0)\n",
        "        a = score / torch.sum(score, dim=-1, keepdim=True)\n",
        "        c = torch.einsum('ijk,ikl->ijl', (a, v))\n",
        "        return c\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, output_dim,maxlen=10000,device='cuda'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.output_dim = output_dim\n",
        "        self.maxlen = maxlen\n",
        "        pe = self.initializer()\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x, mask=None):\n",
        "        pe = self.pe[:x.size(1), :].unsqueeze(0)\n",
        "        return x + pe\n",
        "    def initializer(self):\n",
        "        pe = \\\n",
        "            np.array([[pos / np.power(100, 2 * (i // 2) / self.output_dim)\n",
        "                       for i in range(self.output_dim)]\n",
        "                      for pos in range(self.maxlen)])\n",
        "        pe[:, 0::2] = np.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = np.cos(pe[:, 1::2])\n",
        "        return torch.from_numpy(pe).float()\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,h,d_model,device='cuda'):\n",
        "        super().__init__()\n",
        "        self.h = h\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_k = d_model // h\n",
        "        self.d_v = d_v = d_model // h\n",
        "        self.device = device\n",
        "        self.W_q = nn.Parameter(torch.Tensor(h,d_model, d_k))\n",
        "        self.W_k = nn.Parameter(torch.Tensor(h,d_model,d_k))\n",
        "        self.W_v = nn.Parameter(torch.Tensor(h,d_model,d_v))\n",
        "\n",
        "        nn.init.xavier_normal_(self.W_q)\n",
        "        nn.init.xavier_normal_(self.W_k)\n",
        "        nn.init.xavier_normal_(self.W_v)\n",
        "\n",
        "        self.attn = ScaledDotProductAttention(d_k)\n",
        "        self.linear = nn.Linear((h * d_v), d_model)\n",
        "        nn.init.xavier_normal_(self.linear.weight)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        '''\n",
        "        # Argument\n",
        "            q, k, v: (batch, sequence, out_features)\n",
        "            mask:    (batch, sequence)\n",
        "        '''\n",
        "        batch_size = q.size(0)\n",
        "        q = torch.einsum('hijk,hkl->hijl',(q.unsqueeze(0).repeat(self.h, 1, 1, 1),self.W_q))\n",
        "        k = torch.einsum('hijk,hkl->hijl',(k.unsqueeze(0).repeat(self.h, 1, 1, 1),self.W_k))\n",
        "        v = torch.einsum('hijk,hkl->hijl',(v.unsqueeze(0).repeat(self.h, 1, 1, 1),self.W_v))\n",
        "        q = q.view(-1, q.size(-2), q.size(-1))\n",
        "        k = k.view(-1, k.size(-2), k.size(-1))\n",
        "        v = v.view(-1, v.size(-2), v.size(-1))\n",
        "\n",
        "        if mask is not None:\n",
        "            multiples = [self.h] + [1] * (len(mask.size()) - 1)\n",
        "            mask = mask.repeat(multiples)\n",
        "\n",
        "        c = self.attn(q, k, v, mask=mask)\n",
        "        c = torch.split(c, batch_size, dim=0)\n",
        "        c = torch.cat(c, dim=-1)\n",
        "\n",
        "        out = self.linear(c)\n",
        "\n",
        "        return out\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff,device='cuda'):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(d_model, d_ff)\n",
        "        self.a1 = nn.ReLU()\n",
        "        self.l2 = nn.Linear(d_ff, d_model)\n",
        "    def forward(self, x):\n",
        "        h = self.l1(x)\n",
        "        h = self.a1(h)\n",
        "        y = self.l2(h)\n",
        "        return y\n",
        "\n",
        "class Attentions(nn.Module):\n",
        "    def __init__(self,depth_source,N=6,h=8,d_model=512,d_ff=2048,p_dropout=0.1,maxlen=128,device='cuda'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.embedding = nn.Embedding(depth_source,d_model, padding_idx=0)\n",
        "        self.pe = PositionalEncoding(d_model, maxlen=maxlen)\n",
        "        self.encoder_layers = nn.ModuleList([AttentioLayer(h=h,d_model=d_model,d_ff=d_ff,p_dropout=p_dropout,maxlen=maxlen,device=device) for _ in range(N)])\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embedding(x)\n",
        "        y = self.pe(x)\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            y = encoder_layer(y, mask=mask)\n",
        "        return y\n",
        "class AttentioLayer(nn.Module):\n",
        "    def __init__(self,h=8,d_model=512,d_ff=2048,p_dropout=0.1,maxlen=128,device='cuda'):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(h, d_model)\n",
        "        self.dropout1 = nn.Dropout(p_dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ff = FFN(d_model, d_ff)\n",
        "        self.dropout2 = nn.Dropout(p_dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        h = self.attn(x, x, x, mask=mask)\n",
        "        h = self.dropout1(h)\n",
        "        h = self.norm1(x + h)\n",
        "        y = self.ff(h)\n",
        "        y = self.dropout2(y)\n",
        "        y = self.norm2(h + y)\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "class AttentionNN(nn.Module):\n",
        "    def __init__(self,depth_source,depth_key,N=6,h=8,d_model=512,d_key=224,d_ff=2048,d_off=200,p_dropout=0.3,maxlen=128,device='cuda'):\n",
        "        \"\"\"\n",
        "        (形態素数＝ベクトル数)\n",
        "        depth_source:全ツイートの形態素数\n",
        "        N=6:エンコーダ/デコーダのレイヤを重ねる数\n",
        "        h=8:Multi head attentionにける、attentionの数\n",
        "        d_model=512:埋め込み次元\n",
        "        d_model=512:キーの埋め込み次元\n",
        "        d_ff=2048:フィードフォワード(通常のNNの順伝播)\n",
        "        p_dropout=0.3:ドロップアウト率\n",
        "        maxlen=128:1文の含む最大形態素数\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.embedding_key = nn.Embedding(depth_key,d_key, padding_idx=0)\n",
        "        self.d_model=d_model\n",
        "        self.maxlen = maxlen\n",
        "        self.Attantions = Attentions(depth_source=33550,N=N,h=h,d_model=d_model,d_ff=d_ff,p_dropout=p_dropout,maxlen=maxlen,device=device)\n",
        "        \n",
        "        #Attn model\n",
        "        #self.l1_= nn.Linear(d_model*maxlen, d_off)\n",
        "        #self.a1_= nn.ReLU()\n",
        "        #self.d1_= nn.Dropout(p=p_dropout)\n",
        "\n",
        "        #key model\n",
        "        self.l1__= nn.Linear(d_key, d_off)\n",
        "        self.a1__= nn.ReLU()\n",
        "        self.d1__= nn.Dropout(p=p_dropout)\n",
        "        self.l2__= nn.Linear(d_off, d_off)\n",
        "        self.a2__= nn.ReLU()\n",
        "        self.d2__= nn.Dropout(p=p_dropout)\n",
        "\n",
        "        #concated model\n",
        "        self.l1= nn.Linear(d_key+d_model*maxlen, d_off)\n",
        "        self.a1= nn.ReLU()\n",
        "        self.d1= nn.Dropout(p=p_dropout)\n",
        "        self.l2= nn.Linear(d_off, d_off)\n",
        "        self.a2= nn.ReLU()\n",
        "        self.d2= nn.Dropout(p=p_dropout)\n",
        "        self.l3= nn.Linear(d_off, d_off)\n",
        "        self.a3= nn.ReLU()\n",
        "        self.d3= nn.Dropout(p=p_dropout)\n",
        "\n",
        "\n",
        "        self.linear_out = nn.Linear(d_off, 1)\n",
        "        self.activation_out = nn.Sigmoid()\n",
        "\n",
        "        nn.init.xavier_normal_(self.linear_out.weight)\n",
        "        #nn.init.kaiming_normal_(self.l1_.weight)\n",
        "        nn.init.kaiming_normal_(self.l1.weight)\n",
        "        nn.init.kaiming_normal_(self.l2.weight)\n",
        "        #nn.init.kaiming_normal_(self.l1__.weight)\n",
        "        #nn.init.kaiming_normal_(self.l2__.weight)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.l3.weight)\n",
        "        #nn.init.kaiming_normal_(self.l4.weight)\n",
        "\n",
        "    def sequence_mask(self, x):\n",
        "        return x.eq(0)\n",
        "    def forward(self,source, key):\n",
        "        key=self.embedding_key(key)\n",
        "        mask_source = self.sequence_mask(source)\n",
        "        hs = self.Attantions(source, mask=mask_source)\n",
        "        a_out=hs.reshape(len(source),self.d_model*maxlen)\n",
        "        #a_out=self.l1_(hs.reshape(len(source),self.d_model*maxlen))\n",
        "        #a_out=self.a1_(a_out)\n",
        "        #a_out=self.d1_(a_out)\n",
        "\n",
        "        b_out =key\n",
        "        #b_out =self.l1__(key)\n",
        "        #b_out =self.a1__(b_out)        \n",
        "        #b_out =self.d1__(b_out)\n",
        "        #b_out =self.l2__(b_out)\n",
        "        #b_out =self.a2__(b_out)\n",
        "        #b_out =self.d2__(b_out)       \n",
        "        out = self.l1(torch.cat([a_out,b_out],dim=1))\n",
        "        out = self.a1(out)\n",
        "        out = self.d1(out)\n",
        "        out = self.l2(out)\n",
        "        out = self.a2(out)\n",
        "        out = self.d2(out)\n",
        "        out = self.l3(out)\n",
        "        #out = self.a3(out)\n",
        "        #out = self.d3(out)\n",
        "        #out = self.l4(out)\n",
        "        #out = self.a4(out)\n",
        "        #out = self.d4(out)\n",
        "\n",
        "        #\n",
        "        out = self.linear_out(out)\n",
        "        out = self.activation_out(out)\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf1SNvbLgFNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=AttentionNN(depth_source=27330,depth_key=224,N=5,h=4,d_model=400,d_key=224,d_ff=500,d_off=400,p_dropout=0.2,maxlen=maxlen,device='cuda').to('cuda')\n",
        "optimizer = optimizers.Adam(model.parameters(),lr=0.000001,amsgrad=True)\n",
        "criterion = nn.BCELoss()\n",
        "new=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "144724d8-5da1-495c-d66b-31ebf9b97592",
        "id": "6mffcfrVf5eK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if(new):\n",
        "    training_loss = []\n",
        "    trainaccs=[]\n",
        "    accs=[]\n",
        "    new=False\n",
        "#モデルの学習\n",
        "epochs=3000\n",
        "\n",
        "#テストデータをダウンサンプリング\n",
        "x_test_=np.concatenate([x_test.reshape(len(x_test),maxlen), x_test_key.reshape(len(x_test),1)],1)\n",
        "data=torch.LongTensor(x_test_.astype(np.float32)).to('cuda')\n",
        "a=0\n",
        "for epoch in range(epochs):\n",
        "    \"\"\"\n",
        "    【データローダの設定】\n",
        "    trainデータはダウンサンプリングされるので、そのまま使うと14%ほど、学習データが失われる。\n",
        "    したがって、、エポックごとにraw_trainからサンプリングしたものをデータローダに設定\n",
        "    \"\"\"\n",
        "    X_train, y_train = rus.fit_sample(raw_X_train.reshape(-1, 2) ,raw_y_train.reshape(-1, 1) )\n",
        "    maxlen=158\n",
        "    id_=0\n",
        "    x_train=np.array([[0]*maxlen]*len(X_train))\n",
        "    x_train_key=np.array([0]*len(X_train))\n",
        "    for t in X_train:\n",
        "        tmp=text_voc.encode(t[0])\n",
        "        tmp2=keyword_voc.encode([t[1]])\n",
        "        for i in range(maxlen-len(tmp)):\n",
        "            tmp.append(0)\n",
        "        x_train[id_]=tmp\n",
        "        x_train_key[id_]=tmp2[0]\n",
        "        id_=id_+1\n",
        "    \"\"\"データ水増し\"\"\"\n",
        "    unks=0#unkの量\n",
        "    max_unks=(x_train.size - sum(sum(x_train==0)))*0.25#paddingの量\n",
        "    max_row=x_train.shape[0]\n",
        "    max_colum=x_train.shape[1]\n",
        "    while(unks<max_unks):\n",
        "        row=int(np.random.rand()*max_row)\n",
        "        colum=int(np.random.rand()*max_colum)\n",
        "        if(x_train[row,colum] is not 0):#paddingじゃなければunkに置き換え\n",
        "            x_train[row,colum]=1\n",
        "            unks=unks+1\n",
        "\n",
        "    batch_size=len(x_train)// 25\n",
        "    x_in=np.concatenate([x_train.reshape(len(x_train),maxlen), x_train_key.reshape(len(x_train),1)], 1)\n",
        "    x=torch.LongTensor(x_in.astype(np.float32)).to('cuda')\n",
        "    y=torch.tensor(y_train.reshape(len(y_train),1).astype(np.float32)).to('cuda')\n",
        "    dataset = TensorDataset(x,y)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    #send_line_notification(i)\n",
        "    runnning_loss = 0.0\n",
        "    predis1=np.array([])\n",
        "    predis2=np.array([])\n",
        "    teachers=np.array([])\n",
        "    model.train()\n",
        "    print(\"begin training\")\n",
        "    i=0\n",
        "    for x,y in dataloader:\n",
        "        i=i+1\n",
        "        model.zero_grad()\n",
        "        predi=model(x[:,:-1],x[:,-1])\n",
        "        loss = criterion(predi,y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        runnning_loss += loss.item()\n",
        "        predis1=np.append(predis1,predi.detach().cpu().numpy())\n",
        "        teachers=np.append(teachers,y.cpu())\n",
        "    max_acc=0\n",
        "    acc=0\n",
        "    max_thread=50\n",
        "    bestans=[0]\n",
        "    if(True):#for thread in range(40,99,3):\n",
        "        thread=50\n",
        "        predis2=np.where(predis1 > thread/100, 1, 0)\n",
        "        acc = accuracy_score(teachers,predis2)\n",
        "        if(True):#acc>max_acc):\n",
        "            max_acc=acc\n",
        "            max_thread=thread/100\n",
        "            bestans =predis2\n",
        "    if(epoch%1==0):\n",
        "        print(\"epoch:\",epoch)\n",
        "        print(\"loss:\",runnning_loss)\n",
        "        print(\"*train data acc:\",max_acc,\" thread:\",max_thread)\n",
        "        a=sum(bestans)\n",
        "    trainaccs.append(max_acc)\n",
        "    training_loss.append(runnning_loss)\n",
        "    predis1=np.array([])\n",
        "    predis2=np.array([])\n",
        "    teachers=np.array([])\n",
        "    true_indexes=[]\n",
        "    false_indexes=[]\n",
        "    model.eval()\n",
        "    us_len=sum(y_test)#この大きさに合わせてunder sampling\n",
        "    for j in range(data.shape[0]):\n",
        "        y_pred=model(data[j:j+1,:-1],data[j:j+1,-1])\n",
        "        predis1=np.append(predis1,y_pred.detach().cpu().numpy())\n",
        "        teachers=np.append(teachers,y_test[j])\n",
        "        #アンダーサンプリング用\n",
        "        if((len(true_indexes)<=us_len) and (y_test[j])):\n",
        "            true_indexes.append(j)\n",
        "        elif((len(false_indexes)<=us_len) and (not y_test[j])):\n",
        "            false_indexes.append(j)\n",
        "    c=0\n",
        "    acc=0\n",
        "    acc_us=0\n",
        "    max_thread=0\n",
        "    bestans=[0]\n",
        "    #for thread in range(40,99,3):\n",
        "    predis2=np.where(predis1 > 50/100, 1, 0)\n",
        "    acc = accuracy_score(teachers,predis2)\n",
        "    acc_us1 = accuracy_score(teachers[true_indexes],predis2[true_indexes])\n",
        "    acc_us2 = accuracy_score(teachers[false_indexes],predis2[false_indexes])\n",
        "    #    if(acc>max_acc):\n",
        "    max_acc=acc\n",
        "    max_thread=50/100\n",
        "    bestans=predis2\n",
        "    if(epoch%1==0):\n",
        "        print(\"*test_us data acc:\",(acc_us1 + acc_us2)/2,\" thread:\",max_thread,\" TP:\",acc_us1,\" FN:\",acc_us2)\n",
        "        print(\"mean train:\",a,\"/\",len(x_train),\"=\",a/len(x_train))\n",
        "        print(\"mean test_us:\",sum(predis2[true_indexes])+sum(predis2[false_indexes]),\"/\",len(true_indexes)+len(false_indexes),\"=\",(sum(predis2[true_indexes])+sum(predis2[false_indexes]))/(len(true_indexes)+len(false_indexes)))\n",
        "        print()\n",
        "        print(\"*test data acc:\",max_acc,\" thread:\",max_thread)\n",
        "        print(\"mean test:\",sum(bestans),\"/\",len(x_test),\"=\",sum(bestans)/len(x_test))\n",
        "        print(\"--------------------\")\n",
        "    accs.append(max_acc)\n",
        "    if(max_acc>0.81):#testデータに対し目標値を超えたら切る\n",
        "        break"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "begin training\n",
            "epoch: 0\n",
            "loss: 22.542653381824493\n",
            "*train data acc: 0.4982532751091703  thread: 0.5\n",
            "*test_us data acc: 0.5287566617047736  thread: 0.5  TP: 0.4780835881753313  FN: 0.5794297352342159\n",
            "mean train: 2236 / 4580 = 0.4882096069868996\n",
            "mean test_us: 882 / 1963 = 0.4493122771268467\n",
            "\n",
            "*test data acc: 0.5328371278458844  thread: 0.5\n",
            "mean test: 1024 / 2284 = 0.44833625218914186\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 1\n",
            "loss: 20.2410985827446\n",
            "*train data acc: 0.5172489082969433  thread: 0.5\n",
            "*test_us data acc: 0.5552830666575318  thread: 0.5  TP: 0.5759429153924567  FN: 0.5346232179226069\n",
            "mean train: 2285 / 4580 = 0.49890829694323147\n",
            "mean test_us: 1022 / 1963 = 0.5206316861946001\n",
            "\n",
            "*test data acc: 0.5490367775831874  thread: 0.5\n",
            "mean test: 1179 / 2284 = 0.5161996497373029\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 2\n",
            "loss: 20.091830730438232\n",
            "*train data acc: 0.5024017467248908  thread: 0.5\n",
            "*test_us data acc: 0.5665194707590866  thread: 0.5  TP: 0.6442405708460754  FN: 0.48879837067209775\n",
            "mean train: 2327 / 4580 = 0.5080786026200873\n",
            "mean test_us: 1134 / 1963 = 0.5776872134488028\n",
            "\n",
            "*test data acc: 0.5542907180385289  thread: 0.5\n",
            "mean test: 1301 / 2284 = 0.569614711033275\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 3\n",
            "loss: 18.982586801052094\n",
            "*train data acc: 0.530349344978166  thread: 0.5\n",
            "*test_us data acc: 0.5801911470692651  thread: 0.5  TP: 0.4954128440366973  FN: 0.664969450101833\n",
            "mean train: 2203 / 4580 = 0.4810043668122271\n",
            "mean test_us: 815 / 1963 = 0.4151808456444218\n",
            "\n",
            "*test data acc: 0.589754816112084  thread: 0.5\n",
            "mean test: 928 / 2284 = 0.4063047285464098\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 4\n",
            "loss: 18.68109405040741\n",
            "*train data acc: 0.5336244541484716  thread: 0.5\n",
            "*test_us data acc: 0.5750667987070013  thread: 0.5  TP: 0.43119266055045874  FN: 0.7189409368635438\n",
            "mean train: 2348 / 4580 = 0.5126637554585153\n",
            "mean test_us: 699 / 1963 = 0.3560876209882832\n",
            "\n",
            "*test data acc: 0.5901926444833625  thread: 0.5\n",
            "mean test: 801 / 2284 = 0.35070052539404556\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 5\n",
            "loss: 18.574776977300644\n",
            "*train data acc: 0.52882096069869  thread: 0.5\n",
            "*test_us data acc: 0.5812099960346377  thread: 0.5  TP: 0.4964322120285423  FN: 0.6659877800407332\n",
            "mean train: 2286 / 4580 = 0.49912663755458514\n",
            "mean test_us: 815 / 1963 = 0.4151808456444218\n",
            "\n",
            "*test data acc: 0.589754816112084  thread: 0.5\n",
            "mean test: 930 / 2284 = 0.4071803852889667\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 6\n",
            "loss: 18.545005559921265\n",
            "*train data acc: 0.5399563318777293  thread: 0.5\n",
            "*test_us data acc: 0.5786766278227254  thread: 0.5  TP: 0.5208970438328236  FN: 0.6364562118126272\n",
            "mean train: 2291 / 4580 = 0.5002183406113537\n",
            "mean test_us: 868 / 1963 = 0.4421803362200713\n",
            "\n",
            "*test data acc: 0.5862521891418564  thread: 0.5\n",
            "mean test: 986 / 2284 = 0.4316987740805604\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 7\n",
            "loss: 18.548652291297913\n",
            "*train data acc: 0.5362445414847161  thread: 0.5\n",
            "*test_us data acc: 0.5883424578187186  thread: 0.5  TP: 0.5045871559633027  FN: 0.6720977596741344\n",
            "mean train: 2306 / 4580 = 0.5034934497816594\n",
            "mean test_us: 817 / 1963 = 0.4161996943453897\n",
            "\n",
            "*test data acc: 0.5958844133099825  thread: 0.5\n",
            "mean test: 932 / 2284 = 0.4080560420315236\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 8\n",
            "loss: 18.43586266040802\n",
            "*train data acc: 0.5397379912663756  thread: 0.5\n",
            "*test_us data acc: 0.5786501574726317  thread: 0.5  TP: 0.4689092762487258  FN: 0.6883910386965377\n",
            "mean train: 2262 / 4580 = 0.4938864628820961\n",
            "mean test_us: 766 / 1963 = 0.3902190524707081\n",
            "\n",
            "*test data acc: 0.5880035026269702  thread: 0.5\n",
            "mean test: 880 / 2284 = 0.38528896672504376\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 9\n",
            "loss: 18.09433263540268\n",
            "*train data acc: 0.5430131004366813  thread: 0.5\n",
            "*test_us data acc: 0.583736097875936  thread: 0.5  TP: 0.45769622833843016  FN: 0.709775967413442\n",
            "mean train: 2435 / 4580 = 0.5316593886462883\n",
            "mean test_us: 734 / 1963 = 0.3739174732552216\n",
            "\n",
            "*test data acc: 0.5963222416812609  thread: 0.5\n",
            "mean test: 839 / 2284 = 0.36733800350262696\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 10\n",
            "loss: 18.268778324127197\n",
            "*train data acc: 0.5602620087336244  thread: 0.5\n",
            "*test_us data acc: 0.5760550251105008  thread: 0.5  TP: 0.3720693170234455  FN: 0.780040733197556\n",
            "mean train: 2160 / 4580 = 0.47161572052401746\n",
            "mean test_us: 581 / 1963 = 0.2959755476311768\n",
            "\n",
            "*test data acc: 0.6015761821366025  thread: 0.5\n",
            "mean test: 659 / 2284 = 0.28852889667250436\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 11\n",
            "loss: 18.027584075927734\n",
            "*train data acc: 0.5443231441048035  thread: 0.5\n",
            "*test_us data acc: 0.577013667005072  thread: 0.5  TP: 0.254841997961264  FN: 0.8991853360488798\n",
            "mean train: 2427 / 4580 = 0.5299126637554585\n",
            "mean test_us: 349 / 1963 = 0.17778909831889964\n",
            "\n",
            "*test data acc: 0.6195271453590193  thread: 0.5\n",
            "mean test: 388 / 2284 = 0.16987740805604204\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 12\n",
            "loss: 17.9837247133255\n",
            "*train data acc: 0.5635371179039301  thread: 0.5\n",
            "*test_us data acc: 0.5934626539692031  thread: 0.5  TP: 0.5606523955147809  FN: 0.6262729124236253\n",
            "mean train: 2133 / 4580 = 0.4657205240174673\n",
            "mean test_us: 917 / 1963 = 0.467142129393785\n",
            "\n",
            "*test data acc: 0.5928196147110333  thread: 0.5\n",
            "mean test: 1049 / 2284 = 0.45928196147110334\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 13\n",
            "loss: 17.81953227519989\n",
            "*train data acc: 0.5602620087336244  thread: 0.5\n",
            "*test_us data acc: 0.60311862246222  thread: 0.5  TP: 0.5249745158002038  FN: 0.6812627291242362\n",
            "mean train: 2294 / 4580 = 0.5008733624454148\n",
            "mean test_us: 828 / 1963 = 0.4218033622007132\n",
            "\n",
            "*test data acc: 0.6068301225919439  thread: 0.5\n",
            "mean test: 947 / 2284 = 0.4146234676007005\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 14\n",
            "loss: 18.03807657957077\n",
            "*train data acc: 0.5578602620087336  thread: 0.5\n",
            "*test_us data acc: 0.6061237857375678  thread: 0.5  TP: 0.4271151885830785  FN: 0.785132382892057\n",
            "mean train: 2423 / 4580 = 0.5290393013100436\n",
            "mean test_us: 630 / 1963 = 0.32093734080489045\n",
            "\n",
            "*test data acc: 0.6208406304728546  thread: 0.5\n",
            "mean test: 723 / 2284 = 0.31654991243432573\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 15\n",
            "loss: 18.02053052186966\n",
            "*train data acc: 0.5561135371179039  thread: 0.5\n",
            "*test_us data acc: 0.6004850821411295  thread: 0.5  TP: 0.3527013251783894  FN: 0.8482688391038696\n",
            "mean train: 2293 / 4580 = 0.5006550218340612\n",
            "mean test_us: 495 / 1963 = 0.2521650534895568\n",
            "\n",
            "*test data acc: 0.6317863397548161  thread: 0.5\n",
            "mean test: 552 / 2284 = 0.24168126094570927\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 16\n",
            "loss: 17.92671972513199\n",
            "*train data acc: 0.5475982532751091  thread: 0.5\n",
            "*test_us data acc: 0.60918552289841  thread: 0.5  TP: 0.44036697247706424  FN: 0.7780040733197556\n",
            "mean train: 2246 / 4580 = 0.4903930131004367\n",
            "mean test_us: 650 / 1963 = 0.33112582781456956\n",
            "\n",
            "*test data acc: 0.6274080560420315  thread: 0.5\n",
            "mean test: 734 / 2284 = 0.3213660245183888\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 17\n",
            "loss: 17.79818892478943\n",
            "*train data acc: 0.5495633187772926  thread: 0.5\n",
            "*test_us data acc: 0.6061871069672038  thread: 0.5  TP: 0.5514780835881753  FN: 0.6608961303462322\n",
            "mean train: 2259 / 4580 = 0.49323144104803496\n",
            "mean test_us: 874 / 1963 = 0.44523688232297504\n",
            "\n",
            "*test data acc: 0.6107705779334501  thread: 0.5\n",
            "mean test: 990 / 2284 = 0.43345008756567427\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 18\n",
            "loss: 17.84109276533127\n",
            "*train data acc: 0.5620087336244541  thread: 0.5\n",
            "*test_us data acc: 0.6056561428859117  thread: 0.5  TP: 0.508664627930683  FN: 0.7026476578411406\n",
            "mean train: 2196 / 4580 = 0.4794759825327511\n",
            "mean test_us: 791 / 1963 = 0.40295466123280693\n",
            "\n",
            "*test data acc: 0.6142732049036778  thread: 0.5\n",
            "mean test: 898 / 2284 = 0.39316987740805603\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 19\n",
            "loss: 17.983716785907745\n",
            "*train data acc: 0.5626637554585153  thread: 0.5\n",
            "*test_us data acc: 0.609803164400597  thread: 0.5  TP: 0.6534148827726809  FN: 0.5661914460285132\n",
            "mean train: 2165 / 4580 = 0.47270742358078605\n",
            "mean test_us: 1067 / 1963 = 0.543555781966378\n",
            "\n",
            "*test data acc: 0.5976357267950964  thread: 0.5\n",
            "mean test: 1220 / 2284 = 0.5341506129597198\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 20\n",
            "loss: 17.888247191905975\n",
            "*train data acc: 0.5580786026200873  thread: 0.5\n",
            "*test_us data acc: 0.6082362234803425  thread: 0.5  TP: 0.5759429153924567  FN: 0.6405295315682281\n",
            "mean train: 2328 / 4580 = 0.508296943231441\n",
            "mean test_us: 918 / 1963 = 0.467651553744269\n",
            "\n",
            "*test data acc: 0.6085814360770578  thread: 0.5\n",
            "mean test: 1043 / 2284 = 0.45665499124343256\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 21\n",
            "loss: 17.569633424282074\n",
            "*train data acc: 0.5825327510917031  thread: 0.5\n",
            "*test_us data acc: 0.59153239451825  thread: 0.5  TP: 0.7696228338430173  FN: 0.4134419551934827\n",
            "mean train: 2088 / 4580 = 0.4558951965065502\n",
            "mean test_us: 1331 / 1963 = 0.6780438104941416\n",
            "\n",
            "*test data acc: 0.5586690017513135  thread: 0.5\n",
            "mean test: 1537 / 2284 = 0.6729422066549913\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 22\n",
            "loss: 17.36921626329422\n",
            "*train data acc: 0.5770742358078602  thread: 0.5\n",
            "*test_us data acc: 0.6199766022866231  thread: 0.5  TP: 0.6340468909276249  FN: 0.6059063136456212\n",
            "mean train: 2379 / 4580 = 0.5194323144104803\n",
            "mean test_us: 1009 / 1963 = 0.5140091696383087\n",
            "\n",
            "*test data acc: 0.6129597197898424  thread: 0.5\n",
            "mean test: 1147 / 2284 = 0.5021891418563923\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 23\n",
            "loss: 17.363289654254913\n",
            "*train data acc: 0.584061135371179  thread: 0.5\n",
            "*test_us data acc: 0.618929206865267  thread: 0.5  TP: 0.5769622833843018  FN: 0.6608961303462322\n",
            "mean train: 2371 / 4580 = 0.5176855895196506\n",
            "mean test_us: 899 / 1963 = 0.4579724910850739\n",
            "\n",
            "*test data acc: 0.6234676007005254  thread: 0.5\n",
            "mean test: 1011 / 2284 = 0.4426444833625219\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 24\n",
            "loss: 17.48047697544098\n",
            "*train data acc: 0.5764192139737991  thread: 0.5\n",
            "*test_us data acc: 0.6265853663600258  thread: 0.5  TP: 0.6136595310907238  FN: 0.639511201629328\n",
            "mean train: 2218 / 4580 = 0.48427947598253274\n",
            "mean test_us: 956 / 1963 = 0.4870096790626592\n",
            "\n",
            "*test data acc: 0.6234676007005254  thread: 0.5\n",
            "mean test: 1083 / 2284 = 0.47416812609457093\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 25\n",
            "loss: 17.366427421569824\n",
            "*train data acc: 0.5917030567685589  thread: 0.5\n",
            "*test_us data acc: 0.6088517888766398  thread: 0.5  TP: 0.7849133537206932  FN: 0.43279022403258655\n",
            "mean train: 2168 / 4580 = 0.4733624454148472\n",
            "mean test_us: 1327 / 1963 = 0.6760061130922058\n",
            "\n",
            "*test data acc: 0.5761821366024519  thread: 0.5\n",
            "mean test: 1527 / 2284 = 0.6685639229422067\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 26\n",
            "loss: 17.29998743534088\n",
            "*train data acc: 0.5895196506550219  thread: 0.5\n",
            "*test_us data acc: 0.6199625885718675  thread: 0.5  TP: 0.6065239551478083  FN: 0.6334012219959266\n",
            "mean train: 2296 / 4580 = 0.5013100436681223\n",
            "mean test_us: 955 / 1963 = 0.48650025471217523\n",
            "\n",
            "*test data acc: 0.617338003502627  thread: 0.5\n",
            "mean test: 1083 / 2284 = 0.47416812609457093\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 27\n",
            "loss: 17.657512843608856\n",
            "*train data acc: 0.5930131004366812  thread: 0.5\n",
            "*test_us data acc: 0.6042744944163132  thread: 0.5  TP: 0.7951070336391437  FN: 0.4134419551934827\n",
            "mean train: 2426 / 4580 = 0.5296943231441048\n",
            "mean test_us: 1356 / 1963 = 0.6907794192562404\n",
            "\n",
            "*test data acc: 0.5709281961471103  thread: 0.5\n",
            "mean test: 1559 / 2284 = 0.6825744308231173\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 28\n",
            "loss: 17.39754629135132\n",
            "*train data acc: 0.5810043668122271  thread: 0.5\n",
            "*test_us data acc: 0.6174759327424736  thread: 0.5  TP: 0.7227319062181448  FN: 0.5122199592668024\n",
            "mean train: 2417 / 4580 = 0.5277292576419214\n",
            "mean test_us: 1188 / 1963 = 0.6051961283749363\n",
            "\n",
            "*test data acc: 0.5936952714535902  thread: 0.5\n",
            "mean test: 1365 / 2284 = 0.5976357267950964\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 29\n",
            "loss: 17.32567948102951\n",
            "*train data acc: 0.5899563318777292  thread: 0.5\n",
            "*test_us data acc: 0.61797575523542  thread: 0.5  TP: 0.7043832823649337  FN: 0.5315682281059063\n",
            "mean train: 2428 / 4580 = 0.5301310043668123\n",
            "mean test_us: 1151 / 1963 = 0.58634742740703\n",
            "\n",
            "*test data acc: 0.5971978984238179  thread: 0.5\n",
            "mean test: 1321 / 2284 = 0.5783712784588442\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 30\n",
            "loss: 17.495777368545532\n",
            "*train data acc: 0.5768558951965066  thread: 0.5\n",
            "*test_us data acc: 0.624579848070571  thread: 0.5  TP: 0.6748216106014271  FN: 0.5743380855397149\n",
            "mean train: 2518 / 4580 = 0.5497816593886463\n",
            "mean test_us: 1080 / 1963 = 0.5501782985226694\n",
            "\n",
            "*test data acc: 0.6120840630472855  thread: 0.5\n",
            "mean test: 1229 / 2284 = 0.5380910683012259\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 31\n",
            "loss: 17.490349233150482\n",
            "*train data acc: 0.5958515283842795  thread: 0.5\n",
            "*test_us data acc: 0.615436677732311  thread: 0.5  TP: 0.7176350662589195  FN: 0.5132382892057027\n",
            "mean train: 2265 / 4580 = 0.4945414847161572\n",
            "mean test_us: 1182 / 1963 = 0.6021395822720326\n",
            "\n",
            "*test data acc: 0.5950087565674256  thread: 0.5\n",
            "mean test: 1352 / 2284 = 0.5919439579684763\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 32\n",
            "loss: 17.330218136310577\n",
            "*train data acc: 0.5951965065502184  thread: 0.5\n",
            "*test_us data acc: 0.6301910432639706  thread: 0.5  TP: 0.6952089704383282  FN: 0.5651731160896131\n",
            "mean train: 2220 / 4580 = 0.4847161572052402\n",
            "mean test_us: 1109 / 1963 = 0.564951604686704\n",
            "\n",
            "*test data acc: 0.6142732049036778  thread: 0.5\n",
            "mean test: 1264 / 2284 = 0.553415061295972\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 33\n",
            "loss: 17.407756686210632\n",
            "*train data acc: 0.5943231441048035  thread: 0.5\n",
            "*test_us data acc: 0.6306711427509648  thread: 0.5  TP: 0.6381243628950051  FN: 0.6232179226069247\n",
            "mean train: 2318 / 4580 = 0.506113537117904\n",
            "mean test_us: 996 / 1963 = 0.5073866530820174\n",
            "\n",
            "*test data acc: 0.623029772329247  thread: 0.5\n",
            "mean test: 1132 / 2284 = 0.4956217162872154\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 34\n",
            "loss: 17.2283935546875\n",
            "*train data acc: 0.6041484716157205  thread: 0.5\n",
            "*test_us data acc: 0.6449625366692202  thread: 0.5  TP: 0.7064220183486238  FN: 0.5835030549898167\n",
            "mean train: 2245 / 4580 = 0.490174672489083\n",
            "mean test_us: 1102 / 1963 = 0.5613856342333163\n",
            "\n",
            "*test data acc: 0.6256567425569177  thread: 0.5\n",
            "mean test: 1260 / 2284 = 0.5516637478108581\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 35\n",
            "loss: 17.538336753845215\n",
            "*train data acc: 0.5943231441048035  thread: 0.5\n",
            "*test_us data acc: 0.6312239059440987  thread: 0.5  TP: 0.7237512742099899  FN: 0.5386965376782077\n",
            "mean train: 2378 / 4580 = 0.5192139737991266\n",
            "mean test_us: 1163 / 1963 = 0.5924605196128375\n",
            "\n",
            "*test data acc: 0.612521891418564  thread: 0.5\n",
            "mean test: 1324 / 2284 = 0.5796847635726795\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 36\n",
            "loss: 17.2393581867218\n",
            "*train data acc: 0.6017467248908297  thread: 0.5\n",
            "*test_us data acc: 0.6352572606613228  thread: 0.5  TP: 0.6452599388379205  FN: 0.6252545824847251\n",
            "mean train: 2156 / 4580 = 0.4707423580786026\n",
            "mean test_us: 1001 / 1963 = 0.5099337748344371\n",
            "\n",
            "*test data acc: 0.6287215411558669  thread: 0.5\n",
            "mean test: 1133 / 2284 = 0.4960595446584939\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 37\n",
            "loss: 17.45174068212509\n",
            "*train data acc: 0.6008733624454149  thread: 0.5\n",
            "*test_us data acc: 0.6332730224572374  thread: 0.5  TP: 0.7482161060142711  FN: 0.5183299389002036\n",
            "mean train: 2286 / 4580 = 0.49912663755458514\n",
            "mean test_us: 1207 / 1963 = 0.6148751910341315\n",
            "\n",
            "*test data acc: 0.6081436077057794  thread: 0.5\n",
            "mean test: 1382 / 2284 = 0.6050788091068301\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 38\n",
            "loss: 17.002150386571884\n",
            "*train data acc: 0.5971615720524017  thread: 0.5\n",
            "*test_us data acc: 0.6254632311266404  thread: 0.5  TP: 0.40978593272171254  FN: 0.8411405295315683\n",
            "mean train: 2533 / 4580 = 0.5530567685589519\n",
            "mean test_us: 558 / 1963 = 0.28425878757004586\n",
            "\n",
            "*test data acc: 0.6510507880910683  thread: 0.5\n",
            "mean test: 620 / 2284 = 0.2714535901926445\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 39\n",
            "loss: 17.03852730989456\n",
            "*train data acc: 0.6039301310043668  thread: 0.5\n",
            "*test_us data acc: 0.6341740524133692  thread: 0.5  TP: 0.5178389398572885  FN: 0.7505091649694501\n",
            "mean train: 2344 / 4580 = 0.5117903930131005\n",
            "mean test_us: 753 / 1963 = 0.3835965359144167\n",
            "\n",
            "*test data acc: 0.6457968476357268  thread: 0.5\n",
            "mean test: 844 / 2284 = 0.36952714535901926\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 40\n",
            "loss: 16.854662537574768\n",
            "*train data acc: 0.6122270742358079  thread: 0.5\n",
            "*test_us data acc: 0.6453917715619167  thread: 0.5  TP: 0.5494393476044852  FN: 0.7413441955193483\n",
            "mean train: 2402 / 4580 = 0.5244541484716158\n",
            "mean test_us: 793 / 1963 = 0.40397350993377484\n",
            "\n",
            "*test data acc: 0.6506129597197898  thread: 0.5\n",
            "mean test: 895 / 2284 = 0.39185639229422065\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 41\n",
            "loss: 17.161748111248016\n",
            "*train data acc: 0.6032751091703057  thread: 0.5\n",
            "*test_us data acc: 0.6397541060184233  thread: 0.5  TP: 0.47706422018348627  FN: 0.8024439918533605\n",
            "mean train: 2285 / 4580 = 0.49890829694323147\n",
            "mean test_us: 662 / 1963 = 0.337238920020377\n",
            "\n",
            "*test data acc: 0.6563047285464098  thread: 0.5\n",
            "mean test: 740 / 2284 = 0.3239929947460595\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 42\n",
            "loss: 17.145706832408905\n",
            "*train data acc: 0.6063318777292577  thread: 0.5\n",
            "*test_us data acc: 0.6443900504701342  thread: 0.5  TP: 0.582059123343527  FN: 0.7067209775967414\n",
            "mean train: 2249 / 4580 = 0.49104803493449783\n",
            "mean test_us: 859 / 1963 = 0.43759551706571576\n",
            "\n",
            "*test data acc: 0.6457968476357268  thread: 0.5\n",
            "mean test: 970 / 2284 = 0.4246935201401051\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 43\n",
            "loss: 17.092515110969543\n",
            "*train data acc: 0.6170305676855895  thread: 0.5\n",
            "*test_us data acc: 0.6469125191261256  thread: 0.5  TP: 0.5361875637104995  FN: 0.7576374745417516\n",
            "mean train: 2190 / 4580 = 0.4781659388646288\n",
            "mean test_us: 764 / 1963 = 0.3892002037697402\n",
            "\n",
            "*test data acc: 0.6554290718038529  thread: 0.5\n",
            "mean test: 858 / 2284 = 0.37565674255691767\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 44\n",
            "loss: 16.83160561323166\n",
            "*train data acc: 0.6185589519650655  thread: 0.5\n",
            "*test_us data acc: 0.6356428973303354  thread: 0.5  TP: 0.4026503567787971  FN: 0.8686354378818737\n",
            "mean train: 2299 / 4580 = 0.5019650655021834\n",
            "mean test_us: 524 / 1963 = 0.2669383596535914\n",
            "\n",
            "*test data acc: 0.6641856392294221  thread: 0.5\n",
            "mean test: 576 / 2284 = 0.2521891418563923\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 45\n",
            "loss: 16.913148283958435\n",
            "*train data acc: 0.6155021834061135  thread: 0.5\n",
            "*test_us data acc: 0.6239004424181651  thread: 0.5  TP: 0.34046890927624873  FN: 0.9073319755600815\n",
            "mean train: 2279 / 4580 = 0.49759825327510915\n",
            "mean test_us: 425 / 1963 = 0.21650534895568008\n",
            "\n",
            "*test data acc: 0.6619964973730298  thread: 0.5\n",
            "mean test: 459 / 2284 = 0.2009632224168126\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 46\n",
            "loss: 17.286709368228912\n",
            "*train data acc: 0.6028384279475982  thread: 0.5\n",
            "*test_us data acc: 0.6417746760755786  thread: 0.5  TP: 0.4454638124362895  FN: 0.8380855397148677\n",
            "mean train: 2211 / 4580 = 0.48275109170305674\n",
            "mean test_us: 596 / 1963 = 0.30361691288843606\n",
            "\n",
            "*test data acc: 0.6633099824868651  thread: 0.5\n",
            "mean test: 662 / 2284 = 0.28984238178633975\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 47\n",
            "loss: 16.629693746566772\n",
            "*train data acc: 0.6235807860262009  thread: 0.5\n",
            "*test_us data acc: 0.6474029991425683  thread: 0.5  TP: 0.49949031600407745  FN: 0.7953156822810591\n",
            "mean train: 2272 / 4580 = 0.4960698689956332\n",
            "mean test_us: 691 / 1963 = 0.35201222618441164\n",
            "\n",
            "*test data acc: 0.6633099824868651  thread: 0.5\n",
            "mean test: 768 / 2284 = 0.3362521891418564\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 48\n",
            "loss: 16.609847098588943\n",
            "*train data acc: 0.6207423580786027  thread: 0.5\n",
            "*test_us data acc: 0.6535171309877489  thread: 0.5  TP: 0.5076452599388379  FN: 0.7993890020366599\n",
            "mean train: 2247 / 4580 = 0.4906113537117904\n",
            "mean test_us: 695 / 1963 = 0.3540499235863474\n",
            "\n",
            "*test data acc: 0.6690017513134852  thread: 0.5\n",
            "mean test: 771 / 2284 = 0.3375656742556918\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 49\n",
            "loss: 16.95083522796631\n",
            "*train data acc: 0.6213973799126637  thread: 0.5\n",
            "*test_us data acc: 0.6489294559979737  thread: 0.5  TP: 0.49745158002038736  FN: 0.8004073319755601\n",
            "mean train: 2230 / 4580 = 0.4868995633187773\n",
            "mean test_us: 684 / 1963 = 0.34844625573102395\n",
            "\n",
            "*test data acc: 0.6646234676007006  thread: 0.5\n",
            "mean test: 761 / 2284 = 0.3331873905429072\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 50\n",
            "loss: 16.607292234897614\n",
            "*train data acc: 0.6281659388646288  thread: 0.5\n",
            "*test_us data acc: 0.6479697760504577  thread: 0.5  TP: 0.6126401630988787  FN: 0.6832993890020367\n",
            "mean train: 2305 / 4580 = 0.5032751091703057\n",
            "mean test_us: 912 / 1963 = 0.46459500764136524\n",
            "\n",
            "*test data acc: 0.647985989492119  thread: 0.5\n",
            "mean test: 1025 / 2284 = 0.44877408056042034\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 51\n",
            "loss: 16.483911514282227\n",
            "*train data acc: 0.633406113537118  thread: 0.5\n",
            "*test_us data acc: 0.6570932233827654  thread: 0.5  TP: 0.5310907237512742  FN: 0.7830957230142567\n",
            "mean train: 2219 / 4580 = 0.48449781659388647\n",
            "mean test_us: 734 / 1963 = 0.3739174732552216\n",
            "\n",
            "*test data acc: 0.669877408056042  thread: 0.5\n",
            "mean test: 815 / 2284 = 0.35683012259194397\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 52\n",
            "loss: 16.541158437728882\n",
            "*train data acc: 0.6353711790393013  thread: 0.5\n",
            "*test_us data acc: 0.6561252390116905  thread: 0.5  TP: 0.6299694189602446  FN: 0.6822810590631364\n",
            "mean train: 2338 / 4580 = 0.5104803493449782\n",
            "mean test_us: 930 / 1963 = 0.4737646459500764\n",
            "\n",
            "*test data acc: 0.6558669001751314  thread: 0.5\n",
            "mean test: 1041 / 2284 = 0.45577933450087565\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 53\n",
            "loss: 16.579941153526306\n",
            "*train data acc: 0.6364628820960698  thread: 0.5\n",
            "*test_us data acc: 0.6576646715289066  thread: 0.5  TP: 0.6534148827726809  FN: 0.6619144602851323\n",
            "mean train: 2309 / 4580 = 0.5041484716157205\n",
            "mean test_us: 973 / 1963 = 0.4956698930208864\n",
            "\n",
            "*test data acc: 0.6501751313485113  thread: 0.5\n",
            "mean test: 1100 / 2284 = 0.4816112084063047\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 54\n",
            "loss: 16.42319083213806\n",
            "*train data acc: 0.6314410480349345  thread: 0.5\n",
            "*test_us data acc: 0.6642511174639951  thread: 0.5  TP: 0.5891946992864424  FN: 0.7393075356415478\n",
            "mean train: 2328 / 4580 = 0.508296943231441\n",
            "mean test_us: 834 / 1963 = 0.4248599083036169\n",
            "\n",
            "*test data acc: 0.6681260945709282  thread: 0.5\n",
            "mean test: 933 / 2284 = 0.4084938704028021\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 55\n",
            "loss: 16.20014277100563\n",
            "*train data acc: 0.6419213973799127  thread: 0.5\n",
            "*test_us data acc: 0.6465087165305778  thread: 0.5  TP: 0.7431192660550459  FN: 0.5498981670061099\n",
            "mean train: 2356 / 4580 = 0.514410480349345\n",
            "mean test_us: 1171 / 1963 = 0.5965359144167092\n",
            "\n",
            "*test data acc: 0.6260945709281961  thread: 0.5\n",
            "mean test: 1331 / 2284 = 0.5827495621716288\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 56\n",
            "loss: 16.811916649341583\n",
            "*train data acc: 0.632532751091703  thread: 0.5\n",
            "*test_us data acc: 0.6515956949868271  thread: 0.5  TP: 0.7339449541284404  FN: 0.5692464358452138\n",
            "mean train: 2429 / 4580 = 0.530349344978166\n",
            "mean test_us: 1143 / 1963 = 0.5822720326031584\n",
            "\n",
            "*test data acc: 0.6309106830122592  thread: 0.5\n",
            "mean test: 1302 / 2284 = 0.5700525394045534\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 57\n",
            "loss: 16.568998754024506\n",
            "*train data acc: 0.6344978165938865  thread: 0.5\n",
            "*test_us data acc: 0.6566536079606204  thread: 0.5  TP: 0.6676860346585117  FN: 0.6456211812627292\n",
            "mean train: 2352 / 4580 = 0.5135371179039301\n",
            "mean test_us: 1003 / 1963 = 0.5109526235354049\n",
            "\n",
            "*test data acc: 0.6475481611208407  thread: 0.5\n",
            "mean test: 1134 / 2284 = 0.4964973730297723\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 58\n",
            "loss: 16.331150710582733\n",
            "*train data acc: 0.6436681222707423  thread: 0.5\n",
            "*test_us data acc: 0.6424416250926461  thread: 0.5  TP: 0.7553516819571865  FN: 0.5295315682281059\n",
            "mean train: 2144 / 4580 = 0.46812227074235807\n",
            "mean test_us: 1203 / 1963 = 0.6128374936321956\n",
            "\n",
            "*test data acc: 0.6195271453590193  thread: 0.5\n",
            "mean test: 1370 / 2284 = 0.5998248686514887\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 59\n",
            "loss: 16.591102838516235\n",
            "*train data acc: 0.6427947598253275  thread: 0.5\n",
            "*test_us data acc: 0.638881622518275  thread: 0.5  TP: 0.763506625891947  FN: 0.5142566191446029\n",
            "mean train: 2442 / 4580 = 0.5331877729257642\n",
            "mean test_us: 1226 / 1963 = 0.6245542536933265\n",
            "\n",
            "*test data acc: 0.6160245183887916  thread: 0.5\n",
            "mean test: 1394 / 2284 = 0.6103327495621717\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 60\n",
            "loss: 16.382373332977295\n",
            "*train data acc: 0.6342794759825328  thread: 0.5\n",
            "*test_us data acc: 0.6434620311374362  thread: 0.5  TP: 0.7594291539245668  FN: 0.5274949083503055\n",
            "mean train: 2395 / 4580 = 0.5229257641921398\n",
            "mean test_us: 1209 / 1963 = 0.6158940397350994\n",
            "\n",
            "*test data acc: 0.6199649737302977  thread: 0.5\n",
            "mean test: 1377 / 2284 = 0.6028896672504378\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 61\n",
            "loss: 16.558394074440002\n",
            "*train data acc: 0.6379912663755458  thread: 0.5\n",
            "*test_us data acc: 0.6434485364491531  thread: 0.5  TP: 0.7329255861365953  FN: 0.5539714867617108\n",
            "mean train: 2404 / 4580 = 0.5248908296943231\n",
            "mean test_us: 1157 / 1963 = 0.5894039735099338\n",
            "\n",
            "*test data acc: 0.6265323992994746  thread: 0.5\n",
            "mean test: 1310 / 2284 = 0.5735551663747811\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 62\n",
            "loss: 16.419454514980316\n",
            "*train data acc: 0.6475982532751092  thread: 0.5\n",
            "*test_us data acc: 0.6673689094838593  thread: 0.5  TP: 0.7125382262996942  FN: 0.6221995926680245\n",
            "mean train: 2454 / 4580 = 0.5358078602620088\n",
            "mean test_us: 1070 / 1963 = 0.5450840550178299\n",
            "\n",
            "*test data acc: 0.6554290718038529  thread: 0.5\n",
            "mean test: 1204 / 2284 = 0.5271453590192644\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 63\n",
            "loss: 16.363539576530457\n",
            "*train data acc: 0.6412663755458515  thread: 0.5\n",
            "*test_us data acc: 0.664304577190655  thread: 0.5  TP: 0.6941896024464832  FN: 0.6344195519348269\n",
            "mean train: 2411 / 4580 = 0.5264192139737991\n",
            "mean test_us: 1040 / 1963 = 0.5298013245033113\n",
            "\n",
            "*test data acc: 0.6528021015761821  thread: 0.5\n",
            "mean test: 1174 / 2284 = 0.5140105078809106\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 64\n",
            "loss: 16.452504575252533\n",
            "*train data acc: 0.634061135371179  thread: 0.5\n",
            "*test_us data acc: 0.6424597910191812  thread: 0.5  TP: 0.7910295616717635  FN: 0.4938900203665988\n",
            "mean train: 2220 / 4580 = 0.4847161572052402\n",
            "mean test_us: 1273 / 1963 = 0.6484971981660723\n",
            "\n",
            "*test data acc: 0.6147110332749562  thread: 0.5\n",
            "mean test: 1451 / 2284 = 0.6352889667250438\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 65\n",
            "loss: 16.42622411251068\n",
            "*train data acc: 0.6528384279475983  thread: 0.5\n",
            "*test_us data acc: 0.6617821085346637  thread: 0.5  TP: 0.7400611620795107  FN: 0.5835030549898167\n",
            "mean train: 2376 / 4580 = 0.5187772925764192\n",
            "mean test_us: 1135 / 1963 = 0.5781966377992868\n",
            "\n",
            "*test data acc: 0.6444833625218914  thread: 0.5\n",
            "mean test: 1283 / 2284 = 0.5617338003502627\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 66\n",
            "loss: 16.127605617046356\n",
            "*train data acc: 0.6449781659388646  thread: 0.5\n",
            "*test_us data acc: 0.6506157730068864  thread: 0.5  TP: 0.8093781855249745  FN: 0.49185336048879835\n",
            "mean train: 2370 / 4580 = 0.517467248908297\n",
            "mean test_us: 1293 / 1963 = 0.6586856851757514\n",
            "\n",
            "*test data acc: 0.6177758318739054  thread: 0.5\n",
            "mean test: 1480 / 2284 = 0.647985989492119\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 67\n",
            "loss: 16.516510546207428\n",
            "*train data acc: 0.6316593886462882  thread: 0.5\n",
            "*test_us data acc: 0.6556887377483801  thread: 0.5  TP: 0.7726809378185525  FN: 0.5386965376782077\n",
            "mean train: 2473 / 4580 = 0.5399563318777293\n",
            "mean test_us: 1211 / 1963 = 0.6169128884360673\n",
            "\n",
            "*test data acc: 0.62784588441331  thread: 0.5\n",
            "mean test: 1385 / 2284 = 0.6063922942206655\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 68\n",
            "loss: 16.219966530799866\n",
            "*train data acc: 0.6423580786026201  thread: 0.5\n",
            "*test_us data acc: 0.6688668198832813  thread: 0.5  TP: 0.654434250764526  FN: 0.6832993890020367\n",
            "mean train: 2408 / 4580 = 0.525764192139738\n",
            "mean test_us: 953 / 1963 = 0.4854814060112073\n",
            "\n",
            "*test data acc: 0.6641856392294221  thread: 0.5\n",
            "mean test: 1070 / 2284 = 0.46847635726795095\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 69\n",
            "loss: 15.853593915700912\n",
            "*train data acc: 0.6534934497816594  thread: 0.5\n",
            "*test_us data acc: 0.6718880729792742  thread: 0.5  TP: 0.5881753312945973  FN: 0.7556008146639511\n",
            "mean train: 2283 / 4580 = 0.498471615720524\n",
            "mean test_us: 817 / 1963 = 0.4161996943453897\n",
            "\n",
            "*test data acc: 0.6777583187390543  thread: 0.5\n",
            "mean test: 909 / 2284 = 0.3979859894921191\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 70\n",
            "loss: 16.20678859949112\n",
            "*train data acc: 0.6631004366812228  thread: 0.5\n",
            "*test_us data acc: 0.6724034662663935  thread: 0.5  TP: 0.6004077471967381  FN: 0.7443991853360489\n",
            "mean train: 2281 / 4580 = 0.4980349344978166\n",
            "mean test_us: 840 / 1963 = 0.42791645440652065\n",
            "\n",
            "*test data acc: 0.6764448336252189  thread: 0.5\n",
            "mean test: 936 / 2284 = 0.4098073555166375\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 71\n",
            "loss: 16.134621024131775\n",
            "*train data acc: 0.6548034934497816  thread: 0.5\n",
            "*test_us data acc: 0.6729500011418583  thread: 0.5  TP: 0.6738022426095821  FN: 0.6720977596741344\n",
            "mean train: 2193 / 4580 = 0.47882096069868996\n",
            "mean test_us: 983 / 1963 = 0.5007641365257259\n",
            "\n",
            "*test data acc: 0.6641856392294221  thread: 0.5\n",
            "mean test: 1108 / 2284 = 0.4851138353765324\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 72\n",
            "loss: 16.07271808385849\n",
            "*train data acc: 0.6504366812227074  thread: 0.5\n",
            "*test_us data acc: 0.6673055882542234  thread: 0.5  TP: 0.5881753312945973  FN: 0.7464358452138493\n",
            "mean train: 2289 / 4580 = 0.49978165938864627\n",
            "mean test_us: 826 / 1963 = 0.42078451349974527\n",
            "\n",
            "*test data acc: 0.6738178633975481  thread: 0.5\n",
            "mean test: 918 / 2284 = 0.4019264448336252\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 73\n",
            "loss: 15.920078217983246\n",
            "*train data acc: 0.6665938864628821  thread: 0.5\n",
            "*test_us data acc: 0.6708759713580432  thread: 0.5  TP: 0.6004077471967381  FN: 0.7413441955193483\n",
            "mean train: 2205 / 4580 = 0.4814410480349345\n",
            "mean test_us: 843 / 1963 = 0.42944472745797246\n",
            "\n",
            "*test data acc: 0.6777583187390543  thread: 0.5\n",
            "mean test: 933 / 2284 = 0.4084938704028021\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 74\n",
            "loss: 16.088005304336548\n",
            "*train data acc: 0.6519650655021834  thread: 0.5\n",
            "*test_us data acc: 0.6648116660542154  thread: 0.5  TP: 0.6901121304791029  FN: 0.639511201629328\n",
            "mean train: 2284 / 4580 = 0.49868995633187774\n",
            "mean test_us: 1031 / 1963 = 0.5252165053489557\n",
            "\n",
            "*test data acc: 0.6549912434325744  thread: 0.5\n",
            "mean test: 1161 / 2284 = 0.5083187390542907\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 75\n",
            "loss: 15.880170345306396\n",
            "*train data acc: 0.6554585152838428  thread: 0.5\n",
            "*test_us data acc: 0.6637990454065119  thread: 0.5  TP: 0.7013251783893986  FN: 0.6262729124236253\n",
            "mean train: 2380 / 4580 = 0.519650655021834\n",
            "mean test_us: 1055 / 1963 = 0.5374426897605705\n",
            "\n",
            "*test data acc: 0.6532399299474606  thread: 0.5\n",
            "mean test: 1187 / 2284 = 0.5197022767075307\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 76\n",
            "loss: 15.8158478140831\n",
            "*train data acc: 0.6633187772925764  thread: 0.5\n",
            "*test_us data acc: 0.6678375903884601  thread: 0.5  TP: 0.6330275229357798  FN: 0.7026476578411406\n",
            "mean train: 2360 / 4580 = 0.5152838427947598\n",
            "mean test_us: 913 / 1963 = 0.4651044319918492\n",
            "\n",
            "*test data acc: 0.6681260945709282  thread: 0.5\n",
            "mean test: 1019 / 2284 = 0.44614711033274956\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 77\n",
            "loss: 15.970858097076416\n",
            "*train data acc: 0.6676855895196506  thread: 0.5\n",
            "*test_us data acc: 0.6709143793170027  thread: 0.5  TP: 0.6758409785932722  FN: 0.6659877800407332\n",
            "mean train: 2286 / 4580 = 0.49912663755458514\n",
            "mean test_us: 991 / 1963 = 0.5048395313295976\n",
            "\n",
            "*test data acc: 0.6633099824868651  thread: 0.5\n",
            "mean test: 1114 / 2284 = 0.48774080560420313\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 78\n",
            "loss: 15.66247433423996\n",
            "*train data acc: 0.6729257641921398  thread: 0.5\n",
            "*test_us data acc: 0.6678801505591991  thread: 0.5  TP: 0.7166156982670744  FN: 0.6191446028513238\n",
            "mean train: 2328 / 4580 = 0.508296943231441\n",
            "mean test_us: 1077 / 1963 = 0.5486500254712176\n",
            "\n",
            "*test data acc: 0.6536777583187391  thread: 0.5\n",
            "mean test: 1216 / 2284 = 0.532399299474606\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 79\n",
            "loss: 15.86269748210907\n",
            "*train data acc: 0.6641921397379913  thread: 0.5\n",
            "*test_us data acc: 0.6683047142136438  thread: 0.5  TP: 0.5504587155963303  FN: 0.7861507128309573\n",
            "mean train: 2230 / 4580 = 0.4868995633187773\n",
            "mean test_us: 750 / 1963 = 0.38206826286296486\n",
            "\n",
            "*test data acc: 0.6799474605954466  thread: 0.5\n",
            "mean test: 830 / 2284 = 0.36339754816112085\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 80\n",
            "loss: 15.935940623283386\n",
            "*train data acc: 0.6727074235807861  thread: 0.5\n",
            "*test_us data acc: 0.6749264539488572  thread: 0.5  TP: 0.5555555555555556  FN: 0.7942973523421588\n",
            "mean train: 2221 / 4580 = 0.48493449781659387\n",
            "mean test_us: 747 / 1963 = 0.380539989811513\n",
            "\n",
            "*test data acc: 0.6847635726795096  thread: 0.5\n",
            "mean test: 829 / 2284 = 0.36295971978984237\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 81\n",
            "loss: 15.941116631031036\n",
            "*train data acc: 0.6626637554585153  thread: 0.5\n",
            "*test_us data acc: 0.6677908780059418  thread: 0.5  TP: 0.5412844036697247  FN: 0.7942973523421588\n",
            "mean train: 2211 / 4580 = 0.48275109170305674\n",
            "mean test_us: 733 / 1963 = 0.37340804890473767\n",
            "\n",
            "*test data acc: 0.6799474605954466  thread: 0.5\n",
            "mean test: 812 / 2284 = 0.3555166374781086\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 82\n",
            "loss: 15.804979264736176\n",
            "*train data acc: 0.6707423580786026  thread: 0.5\n",
            "*test_us data acc: 0.6689182035040515  thread: 0.5  TP: 0.7553516819571865  FN: 0.5824847250509165\n",
            "mean train: 2224 / 4580 = 0.485589519650655\n",
            "mean test_us: 1151 / 1963 = 0.58634742740703\n",
            "\n",
            "*test data acc: 0.6475481611208407  thread: 0.5\n",
            "mean test: 1306 / 2284 = 0.5718038528896673\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 83\n",
            "loss: 15.982545852661133\n",
            "*train data acc: 0.6548034934497816  thread: 0.5\n",
            "*test_us data acc: 0.6668815436262511  thread: 0.5  TP: 0.7553516819571865  FN: 0.5784114052953157\n",
            "mean train: 2345 / 4580 = 0.5120087336244541\n",
            "mean test_us: 1155 / 1963 = 0.5883851248089659\n",
            "\n",
            "*test data acc: 0.6453590192644484  thread: 0.5\n",
            "mean test: 1311 / 2284 = 0.5739929947460596\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 84\n",
            "loss: 15.699196517467499\n",
            "*train data acc: 0.6700873362445415  thread: 0.5\n",
            "*test_us data acc: 0.6704514077035986  thread: 0.5  TP: 0.7665647298674821  FN: 0.5743380855397149\n",
            "mean train: 2249 / 4580 = 0.49104803493449783\n",
            "mean test_us: 1170 / 1963 = 0.5960264900662252\n",
            "\n",
            "*test data acc: 0.6471103327495622  thread: 0.5\n",
            "mean test: 1329 / 2284 = 0.5818739054290718\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 85\n",
            "loss: 15.230527758598328\n",
            "*train data acc: 0.6746724890829694  thread: 0.5\n",
            "*test_us data acc: 0.6770165735533176  thread: 0.5  TP: 0.6605504587155964  FN: 0.6934826883910387\n",
            "mean train: 2360 / 4580 = 0.5152838427947598\n",
            "mean test_us: 949 / 1963 = 0.48344370860927155\n",
            "\n",
            "*test data acc: 0.6733800350262698  thread: 0.5\n",
            "mean test: 1061 / 2284 = 0.46453590192644484\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 86\n",
            "loss: 15.411025136709213\n",
            "*train data acc: 0.674235807860262  thread: 0.5\n",
            "*test_us data acc: 0.673953279313058  thread: 0.5  TP: 0.6442405708460754  FN: 0.7036659877800407\n",
            "mean train: 2208 / 4580 = 0.4820960698689956\n",
            "mean test_us: 923 / 1963 = 0.47019867549668876\n",
            "\n",
            "*test data acc: 0.6742556917688266  thread: 0.5\n",
            "mean test: 1027 / 2284 = 0.44964973730297725\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 87\n",
            "loss: 15.318647742271423\n",
            "*train data acc: 0.6816593886462882  thread: 0.5\n",
            "*test_us data acc: 0.6800928434553877  thread: 0.5  TP: 0.7023445463812437  FN: 0.6578411405295316\n",
            "mean train: 2254 / 4580 = 0.49213973799126637\n",
            "mean test_us: 1025 / 1963 = 0.522159959246052\n",
            "\n",
            "*test data acc: 0.670753064798599  thread: 0.5\n",
            "mean test: 1149 / 2284 = 0.5030647985989493\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 88\n",
            "loss: 15.704901158809662\n",
            "*train data acc: 0.6735807860262009  thread: 0.5\n",
            "*test_us data acc: 0.6775070535697603  thread: 0.5  TP: 0.6238532110091743  FN: 0.7311608961303462\n",
            "mean train: 2373 / 4580 = 0.5181222707423581\n",
            "mean test_us: 876 / 1963 = 0.44625573102394295\n",
            "\n",
            "*test data acc: 0.6781961471103327  thread: 0.5\n",
            "mean test: 978 / 2284 = 0.42819614711033277\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 89\n",
            "loss: 15.398383766412735\n",
            "*train data acc: 0.6698689956331878  thread: 0.5\n",
            "*test_us data acc: 0.6800798677935769  thread: 0.5  TP: 0.6768603465851172  FN: 0.6832993890020367\n",
            "mean train: 2190 / 4580 = 0.4781659388646288\n",
            "mean test_us: 975 / 1963 = 0.4966887417218543\n",
            "\n",
            "*test data acc: 0.6716287215411558  thread: 0.5\n",
            "mean test: 1097 / 2284 = 0.48029772329246934\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 90\n",
            "loss: 15.361249208450317\n",
            "*train data acc: 0.6764192139737991  thread: 0.5\n",
            "*test_us data acc: 0.6785897427912413  thread: 0.5  TP: 0.7502548419979612  FN: 0.6069246435845214\n",
            "mean train: 2166 / 4580 = 0.4729257641921397\n",
            "mean test_us: 1122 / 1963 = 0.5715741212429954\n",
            "\n",
            "*test data acc: 0.6615586690017513  thread: 0.5\n",
            "mean test: 1264 / 2284 = 0.553415061295972\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 91\n",
            "loss: 15.54512482881546\n",
            "*train data acc: 0.6792576419213974  thread: 0.5\n",
            "*test_us data acc: 0.6811298583472951  thread: 0.5  TP: 0.7390417940876657  FN: 0.6232179226069247\n",
            "mean train: 2389 / 4580 = 0.5216157205240175\n",
            "mean test_us: 1095 / 1963 = 0.5578196637799286\n",
            "\n",
            "*test data acc: 0.6646234676007006  thread: 0.5\n",
            "mean test: 1235 / 2284 = 0.5407180385288967\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 92\n",
            "loss: 15.183932036161423\n",
            "*train data acc: 0.6818777292576419  thread: 0.5\n",
            "*test_us data acc: 0.6841599348933193  thread: 0.5  TP: 0.6901121304791029  FN: 0.6782077393075356\n",
            "mean train: 2323 / 4580 = 0.5072052401746725\n",
            "mean test_us: 993 / 1963 = 0.5058583800305655\n",
            "\n",
            "*test data acc: 0.6733800350262698  thread: 0.5\n",
            "mean test: 1119 / 2284 = 0.4899299474605954\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 93\n",
            "loss: 15.512007713317871\n",
            "*train data acc: 0.6748908296943231  thread: 0.5\n",
            "*test_us data acc: 0.6795364470769467  thread: 0.5  TP: 0.6095820591233435  FN: 0.7494908350305499\n",
            "mean train: 2327 / 4580 = 0.5080786026200873\n",
            "mean test_us: 844 / 1963 = 0.4299541518084564\n",
            "\n",
            "*test data acc: 0.6838879159369528  thread: 0.5\n",
            "mean test: 937 / 2284 = 0.4102451838879159\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 94\n",
            "loss: 15.380516052246094\n",
            "*train data acc: 0.6759825327510917  thread: 0.5\n",
            "*test_us data acc: 0.6831192868161048  thread: 0.5  TP: 0.6462793068297655  FN: 0.719959266802444\n",
            "mean train: 2226 / 4580 = 0.48602620087336246\n",
            "mean test_us: 909 / 1963 = 0.4630667345899134\n",
            "\n",
            "*test data acc: 0.6790718038528897  thread: 0.5\n",
            "mean test: 1020 / 2284 = 0.44658493870402804\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 95\n",
            "loss: 15.268904328346252\n",
            "*train data acc: 0.6849344978165939  thread: 0.5\n",
            "*test_us data acc: 0.6790189776839377  thread: 0.5  TP: 0.5932721712538226  FN: 0.7647657841140529\n",
            "mean train: 2349 / 4580 = 0.512882096069869\n",
            "mean test_us: 813 / 1963 = 0.4141619969434539\n",
            "\n",
            "*test data acc: 0.6856392294220666  thread: 0.5\n",
            "mean test: 901 / 2284 = 0.3944833625218914\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 96\n",
            "loss: 15.12223595380783\n",
            "*train data acc: 0.6799126637554586  thread: 0.5\n",
            "*test_us data acc: 0.6728903130975292  thread: 0.5  TP: 0.5565749235474006  FN: 0.7892057026476579\n",
            "mean train: 2286 / 4580 = 0.49912663755458514\n",
            "mean test_us: 753 / 1963 = 0.3835965359144167\n",
            "\n",
            "*test data acc: 0.6834500875656743  thread: 0.5\n",
            "mean test: 834 / 2284 = 0.36514886164623467\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 97\n",
            "loss: 15.074135333299637\n",
            "*train data acc: 0.6834061135371179  thread: 0.5\n",
            "*test_us data acc: 0.691279940042062  thread: 0.5  TP: 0.6738022426095821  FN: 0.7087576374745418\n",
            "mean train: 2226 / 4580 = 0.48602620087336246\n",
            "mean test_us: 947 / 1963 = 0.48242485990830364\n",
            "\n",
            "*test data acc: 0.6843257443082311  thread: 0.5\n",
            "mean test: 1062 / 2284 = 0.4649737302977233\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 98\n",
            "loss: 15.905316710472107\n",
            "*train data acc: 0.6879912663755459  thread: 0.5\n",
            "*test_us data acc: 0.688750724041929  thread: 0.5  TP: 0.7064220183486238  FN: 0.6710794297352343\n",
            "mean train: 2343 / 4580 = 0.5115720524017467\n",
            "mean test_us: 1016 / 1963 = 0.5175751400916964\n",
            "\n",
            "*test data acc: 0.6773204903677759  thread: 0.5\n",
            "mean test: 1142 / 2284 = 0.5\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 99\n",
            "loss: 15.052850782871246\n",
            "*train data acc: 0.6921397379912664  thread: 0.5\n",
            "*test_us data acc: 0.6800440549669795  thread: 0.5  TP: 0.6065239551478083  FN: 0.7535641547861507\n",
            "mean train: 2390 / 4580 = 0.5218340611353712\n",
            "mean test_us: 837 / 1963 = 0.4263881813550688\n",
            "\n",
            "*test data acc: 0.6825744308231173  thread: 0.5\n",
            "mean test: 934 / 2284 = 0.4089316987740806\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 100\n",
            "loss: 15.009638279676437\n",
            "*train data acc: 0.6831877729257642  thread: 0.5\n",
            "*test_us data acc: 0.6897571163719634  thread: 0.5  TP: 0.6829765545361876  FN: 0.6965376782077393\n",
            "mean train: 2279 / 4580 = 0.49759825327510915\n",
            "mean test_us: 968 / 1963 = 0.49312277126846665\n",
            "\n",
            "*test data acc: 0.6838879159369528  thread: 0.5\n",
            "mean test: 1081 / 2284 = 0.473292469352014\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 101\n",
            "loss: 15.268622517585754\n",
            "*train data acc: 0.6947598253275109  thread: 0.5\n",
            "*test_us data acc: 0.6790153444986309  thread: 0.5  TP: 0.5861365953109072  FN: 0.7718940936863544\n",
            "mean train: 2360 / 4580 = 0.5152838427947598\n",
            "mean test_us: 799 / 1963 = 0.4070300560366786\n",
            "\n",
            "*test data acc: 0.6852014010507881  thread: 0.5\n",
            "mean test: 888 / 2284 = 0.38879159369527144\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 102\n",
            "loss: 15.042654633522034\n",
            "*train data acc: 0.6884279475982533  thread: 0.5\n",
            "*test_us data acc: 0.6780561835775872  thread: 0.5  TP: 0.7023445463812437  FN: 0.6537678207739308\n",
            "mean train: 2273 / 4580 = 0.4962882096069869\n",
            "mean test_us: 1029 / 1963 = 0.5241976566479878\n",
            "\n",
            "*test data acc: 0.6676882661996497  thread: 0.5\n",
            "mean test: 1156 / 2284 = 0.5061295971978984\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 103\n",
            "loss: 15.21810668706894\n",
            "*train data acc: 0.6995633187772926  thread: 0.5\n",
            "*test_us data acc: 0.6790293582133864  thread: 0.5  TP: 0.6136595310907238  FN: 0.7443991853360489\n",
            "mean train: 2296 / 4580 = 0.5013100436681223\n",
            "mean test_us: 853 / 1963 = 0.434538970962812\n",
            "\n",
            "*test data acc: 0.681260945709282  thread: 0.5\n",
            "mean test: 951 / 2284 = 0.4163747810858144\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 104\n",
            "loss: 15.358207821846008\n",
            "*train data acc: 0.6818777292576419  thread: 0.5\n",
            "*test_us data acc: 0.6785160410321569  thread: 0.5  TP: 0.6055045871559633  FN: 0.7515274949083504\n",
            "mean train: 2143 / 4580 = 0.46790393013100434\n",
            "mean test_us: 838 / 1963 = 0.42689760570555274\n",
            "\n",
            "*test data acc: 0.6834500875656743  thread: 0.5\n",
            "mean test: 930 / 2284 = 0.4071803852889667\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 105\n",
            "loss: 15.00318729877472\n",
            "*train data acc: 0.6917030567685589  thread: 0.5\n",
            "*test_us data acc: 0.688757990412543  thread: 0.5  TP: 0.7206931702344547  FN: 0.6568228105906314\n",
            "mean train: 2258 / 4580 = 0.49301310043668123\n",
            "mean test_us: 1044 / 1963 = 0.5318390219052471\n",
            "\n",
            "*test data acc: 0.6764448336252189  thread: 0.5\n",
            "mean test: 1172 / 2284 = 0.5131348511383538\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 106\n",
            "loss: 15.177841424942017\n",
            "*train data acc: 0.7002183406113537  thread: 0.5\n",
            "*test_us data acc: 0.6872273813453582  thread: 0.5  TP: 0.7145769622833843  FN: 0.659877800407332\n",
            "mean train: 2315 / 4580 = 0.5054585152838428\n",
            "mean test_us: 1035 / 1963 = 0.5272542027508915\n",
            "\n",
            "*test data acc: 0.6773204903677759  thread: 0.5\n",
            "mean test: 1158 / 2284 = 0.5070052539404554\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 107\n",
            "loss: 15.090298771858215\n",
            "*train data acc: 0.7045851528384279  thread: 0.5\n",
            "*test_us data acc: 0.6887553952801808  thread: 0.5  TP: 0.7155963302752294  FN: 0.6619144602851323\n",
            "mean train: 2263 / 4580 = 0.49410480349344976\n",
            "mean test_us: 1034 / 1963 = 0.5267447784004076\n",
            "\n",
            "*test data acc: 0.6773204903677759  thread: 0.5\n",
            "mean test: 1160 / 2284 = 0.5078809106830122\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 108\n",
            "loss: 14.90447187423706\n",
            "*train data acc: 0.6882096069868996  thread: 0.5\n",
            "*test_us data acc: 0.6831166916837426  thread: 0.5  TP: 0.6411824668705403  FN: 0.725050916496945\n",
            "mean train: 2236 / 4580 = 0.4882096069868996\n",
            "mean test_us: 899 / 1963 = 0.4579724910850739\n",
            "\n",
            "*test data acc: 0.6825744308231173  thread: 0.5\n",
            "mean test: 1002 / 2284 = 0.43870402802101577\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 109\n",
            "loss: 14.972698330879211\n",
            "*train data acc: 0.6989082969432314  thread: 0.5\n",
            "*test_us data acc: 0.6836149570972718  thread: 0.5  TP: 0.6197757390417941  FN: 0.7474541751527495\n",
            "mean train: 2261 / 4580 = 0.49366812227074236\n",
            "mean test_us: 856 / 1963 = 0.4360672440142639\n",
            "\n",
            "*test data acc: 0.6865148861646234  thread: 0.5\n",
            "mean test: 951 / 2284 = 0.4163747810858144\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 110\n",
            "loss: 15.211583256721497\n",
            "*train data acc: 0.6930131004366812  thread: 0.5\n",
            "*test_us data acc: 0.678490608735008  thread: 0.5  TP: 0.5555555555555556  FN: 0.8014256619144603\n",
            "mean train: 2206 / 4580 = 0.4816593886462882\n",
            "mean test_us: 740 / 1963 = 0.3769740193581253\n",
            "\n",
            "*test data acc: 0.6913309982486865  thread: 0.5\n",
            "mean test: 814 / 2284 = 0.3563922942206655\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 111\n",
            "loss: 15.015308558940887\n",
            "*train data acc: 0.7013100436681222  thread: 0.5\n",
            "*test_us data acc: 0.6856614784780484  thread: 0.5  TP: 0.6391437308868502  FN: 0.7321792260692465\n",
            "mean train: 2224 / 4580 = 0.485589519650655\n",
            "mean test_us: 890 / 1963 = 0.4533876719307183\n",
            "\n",
            "*test data acc: 0.6852014010507881  thread: 0.5\n",
            "mean test: 992 / 2284 = 0.4343257443082312\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 112\n",
            "loss: 14.870219975709915\n",
            "*train data acc: 0.6936681222707424  thread: 0.5\n",
            "*test_us data acc: 0.6821113374066531  thread: 0.5  TP: 0.6666666666666666  FN: 0.6975560081466395\n",
            "mean train: 2247 / 4580 = 0.4906113537117904\n",
            "mean test_us: 951 / 1963 = 0.4844625573102394\n",
            "\n",
            "*test data acc: 0.6790718038528897  thread: 0.5\n",
            "mean test: 1060 / 2284 = 0.46409807355516636\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 113\n",
            "loss: 14.92759045958519\n",
            "*train data acc: 0.7056768558951965  thread: 0.5\n",
            "*test_us data acc: 0.6754319857330003  thread: 0.5  TP: 0.5484199796126402  FN: 0.8024439918533605\n",
            "mean train: 2238 / 4580 = 0.488646288209607\n",
            "mean test_us: 732 / 1963 = 0.3728986245542537\n",
            "\n",
            "*test data acc: 0.6900175131348512  thread: 0.5\n",
            "mean test: 803 / 2284 = 0.35157618213660247\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 114\n",
            "loss: 14.80312204360962\n",
            "*train data acc: 0.7052401746724891  thread: 0.5\n",
            "*test_us data acc: 0.6866849986816728  thread: 0.5  TP: 0.6493374108053007  FN: 0.7240325865580448\n",
            "mean train: 2252 / 4580 = 0.49170305676855897\n",
            "mean test_us: 908 / 1963 = 0.4625573102394294\n",
            "\n",
            "*test data acc: 0.6838879159369528  thread: 0.5\n",
            "mean test: 1015 / 2284 = 0.44439579684763575\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 115\n",
            "loss: 14.954248130321503\n",
            "*train data acc: 0.6938864628820961  thread: 0.5\n",
            "*test_us data acc: 0.6969009967384376  thread: 0.5  TP: 0.7135575942915392  FN: 0.6802443991853361\n",
            "mean train: 2200 / 4580 = 0.48034934497816595\n",
            "mean test_us: 1014 / 1963 = 0.5165562913907285\n",
            "\n",
            "*test data acc: 0.6852014010507881  thread: 0.5\n",
            "mean test: 1138 / 2284 = 0.4982486865148862\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 116\n",
            "loss: 14.71533340215683\n",
            "*train data acc: 0.6973799126637554  thread: 0.5\n",
            "*test_us data acc: 0.691800004567433  thread: 0.5  TP: 0.6952089704383282  FN: 0.6883910386965377\n",
            "mean train: 2274 / 4580 = 0.4965065502183406\n",
            "mean test_us: 988 / 1963 = 0.5033112582781457\n",
            "\n",
            "*test data acc: 0.6821366024518388  thread: 0.5\n",
            "mean test: 1109 / 2284 = 0.48555166374781084\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 117\n",
            "loss: 14.57117486000061\n",
            "*train data acc: 0.7109170305676856  thread: 0.5\n",
            "*test_us data acc: 0.6918145373086609  thread: 0.5  TP: 0.7237512742099899  FN: 0.659877800407332\n",
            "mean train: 2286 / 4580 = 0.49912663755458514\n",
            "mean test_us: 1044 / 1963 = 0.5318390219052471\n",
            "\n",
            "*test data acc: 0.6781961471103327  thread: 0.5\n",
            "mean test: 1174 / 2284 = 0.5140105078809106\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 118\n",
            "loss: 14.687937200069427\n",
            "*train data acc: 0.7056768558951965  thread: 0.5\n",
            "*test_us data acc: 0.6856287798102854  thread: 0.5  TP: 0.5749235474006116  FN: 0.7963340122199593\n",
            "mean train: 2334 / 4580 = 0.5096069868995633\n",
            "mean test_us: 764 / 1963 = 0.3892002037697402\n",
            "\n",
            "*test data acc: 0.6978984238178634  thread: 0.5\n",
            "mean test: 837 / 2284 = 0.36646234676007006\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 119\n",
            "loss: 14.334959238767624\n",
            "*train data acc: 0.719650655021834  thread: 0.5\n",
            "*test_us data acc: 0.6943261064087313  thread: 0.5  TP: 0.6564729867482161  FN: 0.7321792260692465\n",
            "mean train: 2212 / 4580 = 0.4829694323144105\n",
            "mean test_us: 907 / 1963 = 0.46204788588894546\n",
            "\n",
            "*test data acc: 0.691768826619965  thread: 0.5\n",
            "mean test: 1011 / 2284 = 0.4426444833625219\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 120\n",
            "loss: 14.562730342149734\n",
            "*train data acc: 0.7087336244541484  thread: 0.5\n",
            "*test_us data acc: 0.6922598620220026  thread: 0.5  TP: 0.5983690112130479  FN: 0.7861507128309573\n",
            "mean train: 2268 / 4580 = 0.49519650655021835\n",
            "mean test_us: 797 / 1963 = 0.40601120733571067\n",
            "\n",
            "*test data acc: 0.7005253940455342  thread: 0.5\n",
            "mean test: 877 / 2284 = 0.3839754816112084\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 121\n",
            "loss: 14.627483904361725\n",
            "*train data acc: 0.7067685589519651  thread: 0.5\n",
            "*test_us data acc: 0.680525711533391  thread: 0.5  TP: 0.5524974515800204  FN: 0.8085539714867617\n",
            "mean train: 2201 / 4580 = 0.4805676855895197\n",
            "mean test_us: 730 / 1963 = 0.3718797758532858\n",
            "\n",
            "*test data acc: 0.696584938704028  thread: 0.5\n",
            "mean test: 796 / 2284 = 0.34851138353765326\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 122\n",
            "loss: 14.701239466667175\n",
            "*train data acc: 0.7063318777292577  thread: 0.5\n",
            "*test_us data acc: 0.693336841952287  thread: 0.5  TP: 0.7135575942915392  FN: 0.6731160896130346\n",
            "mean train: 2277 / 4580 = 0.49716157205240175\n",
            "mean test_us: 1021 / 1963 = 0.5201222618441161\n",
            "\n",
            "*test data acc: 0.6821366024518388  thread: 0.5\n",
            "mean test: 1145 / 2284 = 0.5013134851138353\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 123\n",
            "loss: 14.470193833112717\n",
            "*train data acc: 0.7163755458515284  thread: 0.5\n",
            "*test_us data acc: 0.6927825216797358  thread: 0.5  TP: 0.6248725790010193  FN: 0.7606924643584522\n",
            "mean train: 2279 / 4580 = 0.49759825327510915\n",
            "mean test_us: 848 / 1963 = 0.43199184921039224\n",
            "\n",
            "*test data acc: 0.696584938704028  thread: 0.5\n",
            "mean test: 938 / 2284 = 0.4106830122591944\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 124\n",
            "loss: 14.446339666843414\n",
            "*train data acc: 0.7096069868995634  thread: 0.5\n",
            "*test_us data acc: 0.6902507105472407  thread: 0.5  TP: 0.6523955147808359  FN: 0.7281059063136456\n",
            "mean train: 2310 / 4580 = 0.5043668122270742\n",
            "mean test_us: 907 / 1963 = 0.46204788588894546\n",
            "\n",
            "*test data acc: 0.6891418563922942  thread: 0.5\n",
            "mean test: 1009 / 2284 = 0.44176882661996497\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 125\n",
            "loss: 14.878853142261505\n",
            "*train data acc: 0.701528384279476  thread: 0.5\n",
            "*test_us data acc: 0.6820542444946862  thread: 0.5  TP: 0.5545361875637105  FN: 0.8095723014256619\n",
            "mean train: 2229 / 4580 = 0.4866812227074236\n",
            "mean test_us: 731 / 1963 = 0.37238920020376975\n",
            "\n",
            "*test data acc: 0.6948336252189142  thread: 0.5\n",
            "mean test: 804 / 2284 = 0.3520140105078809\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 126\n",
            "loss: 14.753599166870117\n",
            "*train data acc: 0.7141921397379912  thread: 0.5\n",
            "*test_us data acc: 0.6856329320220649  thread: 0.5  TP: 0.583078491335372  FN: 0.7881873727087576\n",
            "mean train: 2253 / 4580 = 0.49192139737991264\n",
            "mean test_us: 780 / 1963 = 0.3973509933774834\n",
            "\n",
            "*test data acc: 0.6957092819614711  thread: 0.5\n",
            "mean test: 858 / 2284 = 0.37565674255691767\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 127\n",
            "loss: 14.33556655049324\n",
            "*train data acc: 0.72117903930131  thread: 0.5\n",
            "*test_us data acc: 0.6978897421684096  thread: 0.5  TP: 0.6554536187563711  FN: 0.7403258655804481\n",
            "mean train: 2213 / 4580 = 0.4831877729257642\n",
            "mean test_us: 898 / 1963 = 0.4574630667345899\n",
            "\n",
            "*test data acc: 0.6948336252189142  thread: 0.5\n",
            "mean test: 1002 / 2284 = 0.43870402802101577\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 128\n",
            "loss: 14.585957884788513\n",
            "*train data acc: 0.7131004366812227  thread: 0.5\n",
            "*test_us data acc: 0.6963809322130666  thread: 0.5  TP: 0.6921508664627931  FN: 0.7006109979633401\n",
            "mean train: 2306 / 4580 = 0.5034934497816594\n",
            "mean test_us: 973 / 1963 = 0.4956698930208864\n",
            "\n",
            "*test data acc: 0.6873905429071804  thread: 0.5\n",
            "mean test: 1091 / 2284 = 0.4776707530647986\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 129\n",
            "loss: 14.615523248910904\n",
            "*train data acc: 0.7135371179039302  thread: 0.5\n",
            "*test_us data acc: 0.6836014624089887  thread: 0.5  TP: 0.5932721712538226  FN: 0.7739307535641547\n",
            "mean train: 2344 / 4580 = 0.5117903930131005\n",
            "mean test_us: 804 / 1963 = 0.4095771777890983\n",
            "\n",
            "*test data acc: 0.6926444833625219  thread: 0.5\n",
            "mean test: 885 / 2284 = 0.38747810858143605\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 130\n",
            "loss: 14.052102595567703\n",
            "*train data acc: 0.724235807860262  thread: 0.5\n",
            "*test_us data acc: 0.6871879353334538  thread: 0.5  TP: 0.6371049949031601  FN: 0.7372708757637475\n",
            "mean train: 2241 / 4580 = 0.4893013100436681\n",
            "mean test_us: 883 / 1963 = 0.44982170147733064\n",
            "\n",
            "*test data acc: 0.6882661996497373  thread: 0.5\n",
            "mean test: 981 / 2284 = 0.4295096322241681\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 131\n",
            "loss: 14.424166977405548\n",
            "*train data acc: 0.7146288209606987  thread: 0.5\n",
            "*test_us data acc: 0.6866761752316415  thread: 0.5  TP: 0.6320081549439348  FN: 0.7413441955193483\n",
            "mean train: 2223 / 4580 = 0.4853711790393013\n",
            "mean test_us: 874 / 1963 = 0.44523688232297504\n",
            "\n",
            "*test data acc: 0.6904553415061296  thread: 0.5\n",
            "mean test: 966 / 2284 = 0.42294220665499127\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 132\n",
            "loss: 14.186118870973587\n",
            "*train data acc: 0.715938864628821  thread: 0.5\n",
            "*test_us data acc: 0.6825597762788294  thread: 0.5  TP: 0.5474006116207951  FN: 0.8177189409368636\n",
            "mean train: 2237 / 4580 = 0.48842794759825325\n",
            "mean test_us: 716 / 1963 = 0.3647478349465104\n",
            "\n",
            "*test data acc: 0.6974605954465849  thread: 0.5\n",
            "mean test: 784 / 2284 = 0.3432574430823117\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 133\n",
            "loss: 14.64004734158516\n",
            "*train data acc: 0.711353711790393  thread: 0.5\n",
            "*test_us data acc: 0.6953439173211591  thread: 0.5  TP: 0.6554536187563711  FN: 0.7352342158859471\n",
            "mean train: 2178 / 4580 = 0.4755458515283843\n",
            "mean test_us: 903 / 1963 = 0.4600101884870097\n",
            "\n",
            "*test data acc: 0.6961471103327496  thread: 0.5\n",
            "mean test: 999 / 2284 = 0.4373905429071804\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 134\n",
            "loss: 14.20014563202858\n",
            "*train data acc: 0.7275109170305677  thread: 0.5\n",
            "*test_us data acc: 0.6994182751297047  thread: 0.5  TP: 0.6574923547400612  FN: 0.7413441955193483\n",
            "mean train: 2186 / 4580 = 0.47729257641921397\n",
            "mean test_us: 899 / 1963 = 0.4579724910850739\n",
            "\n",
            "*test data acc: 0.6974605954465849  thread: 0.5\n",
            "mean test: 1000 / 2284 = 0.43782837127845886\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 135\n",
            "loss: 14.078050017356873\n",
            "*train data acc: 0.7255458515283842  thread: 0.5\n",
            "*test_us data acc: 0.6927747362826493  thread: 0.5  TP: 0.6095820591233435  FN: 0.7759674134419552\n",
            "mean train: 2273 / 4580 = 0.4962882096069869\n",
            "mean test_us: 818 / 1963 = 0.4167091186958737\n",
            "\n",
            "*test data acc: 0.6996497373029772  thread: 0.5\n",
            "mean test: 901 / 2284 = 0.3944833625218914\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 136\n",
            "loss: 14.394007682800293\n",
            "*train data acc: 0.7209606986899564  thread: 0.5\n",
            "*test_us data acc: 0.6902257972765642  thread: 0.5  TP: 0.6034658511722731  FN: 0.7769857433808554\n",
            "mean train: 2158 / 4580 = 0.47117903930131005\n",
            "mean test_us: 811 / 1963 = 0.413143148242486\n",
            "\n",
            "*test data acc: 0.6970227670753065  thread: 0.5\n",
            "mean test: 895 / 2284 = 0.39185639229422065\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 137\n",
            "loss: 14.146999537944794\n",
            "*train data acc: 0.7172489082969432  thread: 0.5\n",
            "*test_us data acc: 0.6953418412152693  thread: 0.5  TP: 0.6513761467889908  FN: 0.7393075356415478\n",
            "mean train: 2313 / 4580 = 0.5050218340611353\n",
            "mean test_us: 895 / 1963 = 0.45593479368313805\n",
            "\n",
            "*test data acc: 0.6948336252189142  thread: 0.5\n",
            "mean test: 994 / 2284 = 0.4352014010507881\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 138\n",
            "loss: 14.164074808359146\n",
            "*train data acc: 0.7220524017467249  thread: 0.5\n",
            "*test_us data acc: 0.6795001152238769  thread: 0.5  TP: 0.5382262996941896  FN: 0.8207739307535642\n",
            "mean train: 2201 / 4580 = 0.4805676855895197\n",
            "mean test_us: 704 / 1963 = 0.358634742740703\n",
            "\n",
            "*test data acc: 0.6970227670753065  thread: 0.5\n",
            "mean test: 767 / 2284 = 0.3358143607705779\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 139\n",
            "loss: 14.24427217245102\n",
            "*train data acc: 0.722707423580786  thread: 0.5\n",
            "*test_us data acc: 0.6968662219647851  thread: 0.5  TP: 0.6452599388379205  FN: 0.7484725050916496\n",
            "mean train: 2158 / 4580 = 0.47117903930131005\n",
            "mean test_us: 880 / 1963 = 0.44829342842587877\n",
            "\n",
            "*test data acc: 0.6978984238178634  thread: 0.5\n",
            "mean test: 975 / 2284 = 0.4268826619964974\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 140\n",
            "loss: 14.194076746702194\n",
            "*train data acc: 0.7131004366812227  thread: 0.5\n",
            "*test_us data acc: 0.6892069483111917  thread: 0.5  TP: 0.6024464831804281  FN: 0.7759674134419552\n",
            "mean train: 2246 / 4580 = 0.4903930131004367\n",
            "mean test_us: 811 / 1963 = 0.413143148242486\n",
            "\n",
            "*test data acc: 0.6987740805604203  thread: 0.5\n",
            "mean test: 889 / 2284 = 0.3892294220665499\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 141\n",
            "loss: 14.303034961223602\n",
            "*train data acc: 0.7115720524017467  thread: 0.5\n",
            "*test_us data acc: 0.6815430034193464  thread: 0.5  TP: 0.5504587155963303  FN: 0.8126272912423625\n",
            "mean train: 2199 / 4580 = 0.4801310043668122\n",
            "mean test_us: 724 / 1963 = 0.36882322975038206\n",
            "\n",
            "*test data acc: 0.6974605954465849  thread: 0.5\n",
            "mean test: 790 / 2284 = 0.3458844133099825\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 142\n",
            "loss: 14.085208803415298\n",
            "*train data acc: 0.7213973799126637  thread: 0.5\n",
            "*test_us data acc: 0.6922738757367581  thread: 0.5  TP: 0.6258919469928644  FN: 0.7586558044806517\n",
            "mean train: 2184 / 4580 = 0.47685589519650656\n",
            "mean test_us: 851 / 1963 = 0.4335201222618441\n",
            "\n",
            "*test data acc: 0.6939579684763573  thread: 0.5\n",
            "mean test: 946 / 2284 = 0.4141856392294221\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 143\n",
            "loss: 14.077682584524155\n",
            "*train data acc: 0.7224890829694323  thread: 0.5\n",
            "*test_us data acc: 0.7009660120704797  thread: 0.5  TP: 0.6972477064220184  FN: 0.7046843177189409\n",
            "mean train: 2211 / 4580 = 0.48275109170305674\n",
            "mean test_us: 974 / 1963 = 0.49617931737137033\n",
            "\n",
            "*test data acc: 0.6922066549912435  thread: 0.5\n",
            "mean test: 1090 / 2284 = 0.47723292469352013\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 144\n",
            "loss: 13.847281575202942\n",
            "*train data acc: 0.7279475982532752  thread: 0.5\n",
            "*test_us data acc: 0.6907183533988968  thread: 0.5  TP: 0.5708460754332314  FN: 0.8105906313645621\n",
            "mean train: 2284 / 4580 = 0.49868995633187774\n",
            "mean test_us: 746 / 1963 = 0.38003056546102904\n",
            "\n",
            "*test data acc: 0.702276707530648  thread: 0.5\n",
            "mean test: 819 / 2284 = 0.3585814360770578\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 145\n",
            "loss: 14.107596427202225\n",
            "*train data acc: 0.72882096069869  thread: 0.5\n",
            "*test_us data acc: 0.7035097608118404  thread: 0.5  TP: 0.6931702344546381  FN: 0.7138492871690427\n",
            "mean train: 2240 / 4580 = 0.4890829694323144\n",
            "mean test_us: 961 / 1963 = 0.48955680081507896\n",
            "\n",
            "*test data acc: 0.6970227670753065  thread: 0.5\n",
            "mean test: 1071 / 2284 = 0.46891418563922943\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 146\n",
            "loss: 14.28417283296585\n",
            "*train data acc: 0.7222707423580786  thread: 0.5\n",
            "*test_us data acc: 0.6928271579563644  thread: 0.5  TP: 0.7125382262996942  FN: 0.6731160896130346\n",
            "mean train: 2334 / 4580 = 0.5096069868995633\n",
            "mean test_us: 1020 / 1963 = 0.5196128374936322\n",
            "\n",
            "*test data acc: 0.6825744308231173  thread: 0.5\n",
            "mean test: 1142 / 2284 = 0.5\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 147\n",
            "loss: 14.056408435106277\n",
            "*train data acc: 0.7205240174672489  thread: 0.5\n",
            "*test_us data acc: 0.6902283924089265  thread: 0.5  TP: 0.6085626911314985  FN: 0.7718940936863544\n",
            "mean train: 2344 / 4580 = 0.5117903930131005\n",
            "mean test_us: 821 / 1963 = 0.41823739174732555\n",
            "\n",
            "*test data acc: 0.6992119089316988  thread: 0.5\n",
            "mean test: 900 / 2284 = 0.39404553415061294\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 148\n",
            "loss: 13.789550721645355\n",
            "*train data acc: 0.725764192139738  thread: 0.5\n",
            "*test_us data acc: 0.695850487158247  thread: 0.5  TP: 0.6503567787971458  FN: 0.7413441955193483\n",
            "mean train: 2204 / 4580 = 0.4812227074235808\n",
            "mean test_us: 892 / 1963 = 0.4544065206316862\n",
            "\n",
            "*test data acc: 0.6983362521891419  thread: 0.5\n",
            "mean test: 984 / 2284 = 0.4308231173380035\n",
            "--------------------\n",
            "begin training\n",
            "epoch: 149\n",
            "loss: 14.048562169075012\n",
            "*train data acc: 0.72882096069869  thread: 0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d6afd7fb9b40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mus_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#この大きさに合わせてunder sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mpredis1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredis1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mteachers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteachers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2bc25eadbe67>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, key)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mmask_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttantions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0ma_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m#a_out=self.l1_(hs.reshape(len(source),self.d_model*maxlen))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2bc25eadbe67>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mencoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAttentioLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2bc25eadbe67>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2bc25eadbe67>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hijk,hkl->hijl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hijk,hkl->hijl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hijk,hkl->hijl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# the old interface of passing the operands as one list argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0moperands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUaZtQWcbotj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(training_loss)\n",
        "plt.ylim(0.,max(training_loss))\n",
        "plt.show()\n",
        "plt.ylim(0.,1.)\n",
        "plt.grid(which='major',color='gray',linestyle='-')\n",
        "plt.plot(trainaccs,color=\"blue\",label=\"train acc\")\n",
        "plt.plot(accs,color=\"orange\",label=\"test acc\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0oHnDDey_HU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_threshold=0\n",
        "max_tacc=0\n",
        "accplot=[]\n",
        "accplot2=[]\n",
        "\n",
        "for threshold in tqdm(range(40,100)):\n",
        "    for j in range(data.shape[0]):\n",
        "        predis2=np.where(predis1 > threshold/100, 1, 0)\n",
        "        acc = accuracy_score(teachers,predis2)\n",
        "        acc_us1 = accuracy_score(teachers[true_indexes],predis2[true_indexes])\n",
        "        acc_us2 = accuracy_score(teachers[false_indexes],predis2[false_indexes])\n",
        "        accplot.append((acc_us1+acc_us2)/2)\n",
        "        accplot2.append(acc)\n",
        "        if((acc_us1+acc_us1)/2>max_tacc):\n",
        "            max_threshold=threshold/100\n",
        "            max_tacc=(acc_us1+acc_us2)/2\n",
        "print(max_tacc,max_threshold)\n",
        "plt.plot(accplot)\n",
        "plt.show()\n",
        "plt.plot(accplot2)\n",
        "plt.show()\n",
        "plt.hist(accplot)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS9wZ-cxbqu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_vali = pd.read_csv('test.csv')\n",
        "keywords_vali = np.array(df_vali.iloc[:,[1]].fillna('nan'))\n",
        "keywords_vali=keywords_vali.reshape(len(keywords_vali))\n",
        "text_vali = np.array(df_vali.iloc[:,[3]])\n",
        "text_vali=text_vali.reshape(len(text_vali))\n",
        "\n",
        "x=np.stack([text_vali, keywords_vali], 1)\n",
        "\n",
        "#testデータをid化\n",
        "maxlen=158\n",
        "id_=0\n",
        "x_vali=np.array([[0]*maxlen]*len(x))\n",
        "x_vali_key=np.array([0]*len(x))\n",
        "for t in text_vali:\n",
        "    tmp=text_voc.encode(t[0])\n",
        "    tmp2=keyword_voc.encode([t[1]])\n",
        "    for i in range(maxlen-len(tmp)):\n",
        "        tmp.append(0)\n",
        "    x_vali[id_]=tmp\n",
        "    x_vali_key[id_]=tmp2[0]\n",
        "    id_=id_+1\n",
        "\n",
        "x_vali_=np.concatenate([x_vali.reshape(len(x_vali),maxlen), x_vali_key.reshape(len(x_vali),1)],1)\n",
        "data=torch.LongTensor(x_vali_.astype(np.float32)).to('cuda')\n",
        "predis1=np.array([])\n",
        "model.eval()\n",
        "for j in range(data.shape[0]):\n",
        "    y_pred=model(data[j:j+1,:-1],data[j:j+1,-1])\n",
        "    predis1=np.append(predis1,y_pred.detach().cpu().numpy())\n",
        "print(\"threshold is:\",max_threshold)\n",
        "predis2=np.where(predis1 > max_threshold, 1, 0)\n",
        "plt.hist(predis2)\n",
        "plt.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKv1qV8W4vxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv = pd.read_csv('sample_submission.csv')\n",
        "csv['target']=np.array(predis2).reshape(len(predis2))\n",
        "csv.to_csv('submission.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}